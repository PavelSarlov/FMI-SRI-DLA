#############################################################################
### Търсене и извличане на информация. Приложение на дълбоко машинно обучение
### Стоян Михов
### Зимен семестър 2021/2022
#############################################################################
###
### Домашно задание 2
###
#############################################################################

import numpy as np

#############################################################

def sigmoid(x):
    return 1/(1+np.exp(-x))

def lossAndGradient(u_w, Vt, v):
    ###  Векторът u_w е влагането на целевата дума. shape(u_w) = M.
    ###  Матрицата Vt представя влаганията на контекстните думи. shape(Vt) = (n+1)xM.
    ###  Векторът v е параметър. shape(v) = M.
    ###  Първият ред на Vt е влагането на коректната контекстна дума, а
    ###  следващите n реда са влаганията на извадката от негативни контекстни думи
    ###
    ###  функцията връща J -- загубата в тази точка;
    ###                  du_w -- градиентът на J спрямо u_w;
    ###                  dVt --  градиентът на J спрямо Vt;
    ###                  dv -- градиентът на J спрямо v.
    #############################################################################
    #### Забележка: За по-добра числена стабилност използвайте np.tanh,
    ####            вместо сами да имплементирате хиперболичен тангенс.
    #### Начало на Вашия код. На мястото на pass се очакват 7-15 реда
    
    delta = np.array([1 if x==0 else 0 for x in range(Vt.shape[0])])
    U_W = np.tanh(u_w + Vt)
    vU_W = np.dot(v, U_W.T) 

    J = -(np.dot(delta, np.log(sigmoid(vU_W))) + np.dot(1-delta, np.log(sigmoid(-vU_W))))
    dv = -(np.dot(delta, ((1-sigmoid(vU_W)) * U_W.T).T)
          +np.dot(1-delta, ((1-sigmoid(-vU_W)) * -U_W.T).T))
    du_w = -(np.dot(delta, ((1-sigmoid(vU_W)) * (v * (1 - U_W**2)).T).T)
            +np.dot(1-delta, ((1-sigmoid(-vU_W)) * (-v * (1 - U_W**2)).T).T))
    dVt = -((delta*((1-sigmoid(vU_W)) * (v * (1 - U_W**2)).T)).T
            +((1-delta)*((1-sigmoid(-vU_W)) * (-v * (1 - U_W**2)).T)).T)

    #### Край на Вашия код
    #############################################################################

    return J, du_w, dVt, dv


def lossAndGradientCumulative(u_w, Vt, v):
    ###  Изчисляване на загуба и градиент за цяла партида
    ###  Тук за всяко от наблюденията се извиква lossAndGradient
    ###  и се акумулират загубата и градиентите за S-те наблюдения
    Cdu_w = []
    CdVt = []
    Cdv = []
    CJ = 0
    S = u_w.shape[0]
    for i in range(S):
        J, du_w, dVt, dv = lossAndGradient(u_w[i],Vt[i], v)
        Cdu_w.append(du_w/S)
        CdVt.append(dVt/S)
        Cdv.append(dv/S)
        CJ += J/S
    return CJ, Cdu_w, CdVt, Cdv


def lossAndGradientBatched(u_w, Vt, v):
    ###  Изчисляване на загуба и градиент за цяла партида.
    ###  Тук едновременно се изчислява загубата и градиентите за S наблюдения.
    ###  Матрицата u_w представя влаганията на целевите думи и shape(u_w) = SxM.
    ###  Тензорът Vt представя S матрици от влагания на контекстните думи и shape(Vt) = Sx(n+1)xM.
    ###  Параметричният вектор v; shape(v) = M.
    ###  Във всяка от S-те матрици на Vt в първия ред е влагането на коректната контекстна дума, а
    ###  следващите n реда са влаганията на извадката от негативни контекстни думи.
    ###
    ###  Функцията връща J -- загубата за цялата партида;
    ###                  du_w -- матрица с размерност SxM с градиентите на J спрямо u_w за всяко наблюдение;
    ###                  dVt --  с размерност Sx(n+1)xM -- S градиента на J спрямо Vt;
    ###                  dv -- с размерност SxM -- S градиента на J спрямо v.
    #############################################################
    ###  От вас се очаква вместо да акумулирате резултатите за отделните наблюдения,
    ###  да използвате тензорни операции, чрез които наведнъж да получите
    ###  резултата за цялата партида. Очаква се по този начин да получите над 2 пъти по-бързо изпълнение.
    #############################################################

    #############################################################################
    #### Начало на Вашия код. На мястото на pass се очакват 10-20 реда
    
    delta = np.array([1 if x==0 else 0 for x in range(Vt.shape[1])])
    S = u_w.shape[0]
    U_W = np.tanh(np.expand_dims(u_w, 1) + Vt)
    vU_W = np.tensordot(v, U_W, (0,2)) 

    J = np.sum(-(np.dot(delta, np.log(sigmoid(vU_W).T))
         +np.dot(1-delta, np.log(sigmoid(-vU_W).T))) / S, 0)
    dv = -(np.dot(delta, ((1-sigmoid(vU_W)).T * U_W.T).T)
          +np.dot(1-delta, ((1-sigmoid(-vU_W)).T * -U_W.T).T)) / S
    du_w = -(np.dot(delta, ((1-sigmoid(vU_W)).T * (v * (1 - U_W**2)).T).T)
            +np.dot(1-delta, ((1-sigmoid(-vU_W)).T * (-v * (1 - U_W**2)).T).T)) / S
    delta = np.broadcast_to(delta, (Vt.shape[0], Vt.shape[1])).T
    dVt = -((delta*((1-sigmoid(vU_W)).T * (v * (1 - U_W**2)).T)).T
            +((1-delta)*((1-sigmoid(-vU_W)).T * (-v * (1 - U_W**2)).T)).T) / S

    #### Край на Вашия код
    #############################################################################
    return J, du_w, dVt, dv
    
