{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Търсене и извличане на информация. Приложение на дълбоко машинно обучение\n",
    "> ### Стоян Михов\n",
    "> #### Зимен семестър 2021/2022\n",
    "\n",
    "### Упражнение 13\n",
    "\n",
    " За да работи програмата трябва корпуса от публицистични текстове за Югоизточна Европа,\n",
    " да се намира разархивиран в директорията, в която е програмата (виж упражнение 2).\n",
    "\n",
    " Преди да се стартира програмата е необходимо да се активира съответното обкръжение с командата: `conda activate tii`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Визуализация на прогреса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class progressBar:\n",
    "    def __init__(self ,barWidth = 50):\n",
    "        self.barWidth = barWidth\n",
    "        self.period = None\n",
    "    def start(self, count):\n",
    "        self.item=0\n",
    "        self.period = int(count / self.barWidth)\n",
    "        sys.stdout.write(\"[\"+(\" \" * self.barWidth)+\"]\")\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"\\b\" * (self.barWidth+1))\n",
    "    def tick(self):\n",
    "        if self.item>0 and self.item % self.period == 0:\n",
    "            sys.stdout.write(\"-\")\n",
    "            sys.stdout.flush()\n",
    "        self.item += 1\n",
    "    def stop(self):\n",
    "        sys.stdout.write(\"]\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractDictionary(corpus, limit=20000):\n",
    "    pb = progressBar()\n",
    "    pb.start(len(corpus))\n",
    "    dictionary = {}\n",
    "    for doc in corpus:\n",
    "        pb.tick()\n",
    "        for w in doc:\n",
    "            if w not in dictionary: dictionary[w] = 0\n",
    "        dictionary[w] += 1\n",
    "    L = sorted([(w,dictionary[w]) for w in dictionary], key = lambda x: x[1] , reverse=True)\n",
    "    if limit > len(L): limit = len(L)\n",
    "    words = [ w for w,_ in L[:limit] ] + [unkToken] + [padToken]\n",
    "    word2ind = { w:i for i,w in enumerate(words)}\n",
    "    pb.stop()\n",
    "    return words, word2ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitSentCorpus(fullSentCorpus, testFraction = 0.1):\n",
    "    random.seed(42)\n",
    "    random.shuffle(fullSentCorpus)\n",
    "    testCount = int(len(fullSentCorpus) * testFraction)\n",
    "    testSentCorpus = fullSentCorpus[:testCount]\n",
    "    trainSentCorpus = fullSentCorpus[testCount:]\n",
    "    return testSentCorpus, trainSentCorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######   Зареждане на корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_root = '../../JOURNALISM.BG/C-MassMedia'\n",
    "myCorpus = PlaintextCorpusReader(corpus_root, '.*\\.txt')\n",
    "startToken = '<s>'\n",
    "endToken = '</s>'\n",
    "unkToken = '<unk>'\n",
    "padToken = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                                  --------------------------------------------------]\n"
     ]
    }
   ],
   "source": [
    "corpus = [ [startToken] + [w.lower() for w in sent] + [endToken] for sent in myCorpus.sents()]\n",
    "words, word2ind = extractDictionary(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCorpus, trainCorpus  = splitSentCorpus(corpus, testFraction = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 32\n",
    "emb_size = 50\n",
    "hid_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 18 23:10:36 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 511.23       Driver Version: 511.23       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   42C    P0    N/A /  75W |    503MiB /  4096MiB |      3%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      3120    C+G   ...8bbwe\\WindowsTerminal.exe    N/A      |\n",
      "|    0   N/A  N/A      6564    C+G   ...nputApp\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A      7732    C+G   ...owser\\Browser\\firefox.exe    N/A      |\n",
      "|    0   N/A  N/A      8508    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9528    C+G   ...in7x64\\steamwebhelper.exe    N/A      |\n",
      "|    0   N/A  N/A     10396    C+G   ...\\app-1.0.9003\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A     12284    C+G   ...owser\\Browser\\firefox.exe    N/A      |\n",
      "|    0   N/A  N/A     13084    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     13492    C+G                                   N/A      |\n",
      "|    0   N/A  N/A     14064    C+G   ...owser\\Browser\\firefox.exe    N/A      |\n",
      "|    0   N/A  N/A     14608    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "#device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSTM с пакетиране на партида"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModelPack(torch.nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, word2ind, unkToken, padToken):\n",
    "        super(LSTMLanguageModelPack, self).__init__()\n",
    "        self.word2ind = word2ind\n",
    "        self.unkTokenIdx = word2ind[unkToken]\n",
    "        self.padTokenIdx = word2ind[padToken]\n",
    "        self.lstm = torch.nn.LSTM(embed_size, hidden_size)\n",
    "        self.embed = torch.nn.Embedding(len(word2ind), embed_size)\n",
    "        self.projection = torch.nn.Linear(hidden_size,len(word2ind))\n",
    "    \n",
    "    def preparePaddedBatch(self, source):\n",
    "        device = next(self.parameters()).device\n",
    "        m = max(len(s) for s in source)\n",
    "        sents = [[self.word2ind.get(w,self.unkTokenIdx) for w in s] for s in source]\n",
    "        sents_padded = [ s+(m-len(s))*[self.padTokenIdx] for s in sents]\n",
    "        return torch.t(torch.tensor(sents_padded, dtype=torch.long, device=device)) # (w,s)\n",
    "    \n",
    "    def forward(self, source):\n",
    "        X = self.preparePaddedBatch(source) # (w,s)\n",
    "        E = self.embed(X[:-1]) # (w,s,e) # cntg\n",
    "        source_lengths = [len(s)-1 for s in source]\n",
    "        outputPacked, _ = self.lstm(torch.nn.utils.rnn.pack_padded_sequence(E, source_lengths,enforce_sorted=False))\n",
    "        output,_ = torch.nn.utils.rnn.pad_packed_sequence(outputPacked) # (w,s,h)\n",
    "\n",
    "        Z = self.projection(output.flatten(0,1)) # (w*s,h)\n",
    "        Y_bar = X[1:].flatten(0,1) # (w*s)\n",
    "        H = torch.nn.functional.cross_entropy(Z,Y_bar,ignore_index=self.padTokenIdx)\n",
    "        return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LSTMLanguageModelPack(emb_size, hid_size, word2ind, unkToken, padToken).to(device) # i n\n",
    "optimizer = torch.optim.Adam(lm.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(len(trainCorpus), dtype='int32')\n",
    "np.random.shuffle(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 388463 9.89737319946289\n",
      "160 / 388463 6.993530750274658\n",
      "320 / 388463 7.019619941711426\n",
      "480 / 388463 6.388062000274658\n",
      "640 / 388463 6.440225124359131\n",
      "800 / 388463 6.282516002655029\n",
      "960 / 388463 5.968737602233887\n",
      "1120 / 388463 6.178872585296631\n",
      "1280 / 388463 6.01486873626709\n",
      "1440 / 388463 5.940061569213867\n",
      "1600 / 388463 5.865983963012695\n",
      "1760 / 388463 5.653263092041016\n",
      "1920 / 388463 5.64120626449585\n",
      "2080 / 388463 5.555856704711914\n",
      "2240 / 388463 5.644376277923584\n",
      "2400 / 388463 5.311214447021484\n",
      "2560 / 388463 5.380514621734619\n",
      "2720 / 388463 5.682637691497803\n",
      "2880 / 388463 5.534104347229004\n",
      "3040 / 388463 5.559726715087891\n",
      "3200 / 388463 5.400074005126953\n",
      "3360 / 388463 5.478166580200195\n",
      "3520 / 388463 5.323108673095703\n",
      "3680 / 388463 5.172706127166748\n",
      "3840 / 388463 5.278953552246094\n",
      "4000 / 388463 5.090494632720947\n",
      "4160 / 388463 5.277441501617432\n",
      "4320 / 388463 5.009152889251709\n",
      "4480 / 388463 4.942946910858154\n",
      "4640 / 388463 4.858317852020264\n",
      "4800 / 388463 4.846898555755615\n",
      "4960 / 388463 5.15395450592041\n",
      "5120 / 388463 4.976353645324707\n",
      "5280 / 388463 4.9295268058776855\n",
      "5440 / 388463 4.9106011390686035\n",
      "5600 / 388463 4.824061393737793\n",
      "5760 / 388463 4.676127910614014\n",
      "5920 / 388463 4.827531814575195\n",
      "6080 / 388463 5.0958428382873535\n",
      "6240 / 388463 4.717701435089111\n",
      "6400 / 388463 4.961965084075928\n",
      "6560 / 388463 4.608032703399658\n",
      "6720 / 388463 4.5134711265563965\n",
      "6880 / 388463 4.589127540588379\n",
      "7040 / 388463 4.924380302429199\n",
      "7200 / 388463 4.704959392547607\n",
      "7360 / 388463 4.46636962890625\n",
      "7520 / 388463 4.831275463104248\n",
      "7680 / 388463 4.415994167327881\n",
      "7840 / 388463 4.921645164489746\n",
      "8000 / 388463 4.434535503387451\n",
      "8160 / 388463 4.484455108642578\n",
      "8320 / 388463 4.795273303985596\n",
      "8480 / 388463 4.344207763671875\n",
      "8640 / 388463 4.611061096191406\n",
      "8800 / 388463 4.568124771118164\n",
      "8960 / 388463 4.529111862182617\n",
      "9120 / 388463 4.622840881347656\n",
      "9280 / 388463 4.5855913162231445\n",
      "9440 / 388463 4.7841081619262695\n",
      "9600 / 388463 4.7342095375061035\n",
      "9760 / 388463 4.65903377532959\n",
      "9920 / 388463 4.565367698669434\n",
      "10080 / 388463 4.7210893630981445\n",
      "10240 / 388463 4.60093355178833\n",
      "10400 / 388463 4.562924385070801\n",
      "10560 / 388463 4.524173259735107\n",
      "10720 / 388463 4.534616947174072\n",
      "10880 / 388463 4.495686054229736\n",
      "11040 / 388463 4.366013050079346\n",
      "11200 / 388463 4.637880325317383\n",
      "11360 / 388463 4.698486328125\n",
      "11520 / 388463 4.559340000152588\n",
      "11680 / 388463 4.636446952819824\n",
      "11840 / 388463 4.46407413482666\n",
      "12000 / 388463 4.530963897705078\n",
      "12160 / 388463 4.534041404724121\n",
      "12320 / 388463 4.437215805053711\n",
      "12480 / 388463 4.391885757446289\n",
      "12640 / 388463 4.617747783660889\n",
      "12800 / 388463 4.531476974487305\n",
      "12960 / 388463 4.669155597686768\n",
      "13120 / 388463 4.268959045410156\n",
      "13280 / 388463 4.316189289093018\n",
      "13440 / 388463 4.1517839431762695\n",
      "13600 / 388463 4.284126281738281\n",
      "13760 / 388463 4.47379207611084\n",
      "13920 / 388463 4.328292369842529\n",
      "14080 / 388463 4.243553161621094\n",
      "14240 / 388463 4.556542873382568\n",
      "14400 / 388463 4.447369575500488\n",
      "14560 / 388463 4.4840087890625\n",
      "14720 / 388463 4.22968053817749\n",
      "14880 / 388463 4.0333638191223145\n",
      "15040 / 388463 4.485600471496582\n",
      "15200 / 388463 4.594827651977539\n",
      "15360 / 388463 4.25478458404541\n",
      "15520 / 388463 4.3988261222839355\n",
      "15680 / 388463 4.292381763458252\n",
      "15840 / 388463 4.5020880699157715\n",
      "16000 / 388463 4.440105438232422\n",
      "16160 / 388463 4.143032550811768\n",
      "16320 / 388463 4.598054885864258\n",
      "16480 / 388463 4.405228137969971\n",
      "16640 / 388463 4.021885395050049\n",
      "16800 / 388463 4.5070343017578125\n",
      "16960 / 388463 4.461482524871826\n",
      "17120 / 388463 4.679082870483398\n",
      "17280 / 388463 4.114124298095703\n",
      "17440 / 388463 4.306443691253662\n",
      "17600 / 388463 4.330205917358398\n",
      "17760 / 388463 4.1521525382995605\n",
      "17920 / 388463 4.215977191925049\n",
      "18080 / 388463 4.248033046722412\n",
      "18240 / 388463 4.250186920166016\n",
      "18400 / 388463 4.355134963989258\n",
      "18560 / 388463 4.433417320251465\n",
      "18720 / 388463 4.109653949737549\n",
      "18880 / 388463 4.144399166107178\n",
      "19040 / 388463 3.843417167663574\n",
      "19200 / 388463 4.259861946105957\n",
      "19360 / 388463 4.159092426300049\n",
      "19520 / 388463 4.020396709442139\n",
      "19680 / 388463 4.370419502258301\n",
      "19840 / 388463 4.398620128631592\n",
      "20000 / 388463 4.36512565612793\n",
      "20160 / 388463 4.328311443328857\n",
      "20320 / 388463 4.286317825317383\n",
      "20480 / 388463 4.331022262573242\n",
      "20640 / 388463 3.957303762435913\n",
      "20800 / 388463 4.026680946350098\n",
      "20960 / 388463 4.034481048583984\n",
      "21120 / 388463 4.101439952850342\n",
      "21280 / 388463 3.8232126235961914\n",
      "21440 / 388463 4.044787406921387\n",
      "21600 / 388463 3.969897508621216\n",
      "21760 / 388463 4.430234432220459\n",
      "21920 / 388463 3.9855170249938965\n",
      "22080 / 388463 4.171442031860352\n",
      "22240 / 388463 4.451893329620361\n",
      "22400 / 388463 4.039881706237793\n",
      "22560 / 388463 4.071590900421143\n",
      "22720 / 388463 4.373207092285156\n",
      "22880 / 388463 4.091213226318359\n",
      "23040 / 388463 3.7652206420898438\n",
      "23200 / 388463 4.368234157562256\n",
      "23360 / 388463 4.230129718780518\n",
      "23520 / 388463 4.19296932220459\n",
      "23680 / 388463 4.205739498138428\n",
      "23840 / 388463 4.3081440925598145\n",
      "24000 / 388463 3.859079122543335\n",
      "24160 / 388463 4.045358180999756\n",
      "24320 / 388463 4.183291912078857\n",
      "24480 / 388463 4.360189437866211\n",
      "24640 / 388463 4.012836933135986\n",
      "24800 / 388463 4.25166130065918\n",
      "24960 / 388463 3.985280752182007\n",
      "25120 / 388463 4.168284893035889\n",
      "25280 / 388463 4.1843791007995605\n",
      "25440 / 388463 4.022352695465088\n",
      "25600 / 388463 4.171594142913818\n",
      "25760 / 388463 4.019024848937988\n",
      "25920 / 388463 4.4352498054504395\n",
      "26080 / 388463 3.8975720405578613\n",
      "26240 / 388463 4.342926979064941\n",
      "26400 / 388463 4.317218780517578\n",
      "26560 / 388463 4.053374767303467\n",
      "26720 / 388463 4.258459568023682\n",
      "26880 / 388463 4.215151786804199\n",
      "27040 / 388463 4.154377460479736\n",
      "27200 / 388463 3.9690656661987305\n",
      "27360 / 388463 4.2063727378845215\n",
      "27520 / 388463 4.237950801849365\n",
      "27680 / 388463 4.158780097961426\n",
      "27840 / 388463 3.7969813346862793\n",
      "28000 / 388463 4.162858963012695\n",
      "28160 / 388463 3.8831164836883545\n",
      "28320 / 388463 4.001775741577148\n",
      "28480 / 388463 4.174832344055176\n",
      "28640 / 388463 4.206990718841553\n",
      "28800 / 388463 3.902714490890503\n",
      "28960 / 388463 4.18333101272583\n",
      "29120 / 388463 4.055667400360107\n",
      "29280 / 388463 3.917046546936035\n",
      "29440 / 388463 4.01651668548584\n",
      "29600 / 388463 4.132739543914795\n",
      "29760 / 388463 3.9482333660125732\n",
      "29920 / 388463 4.201168060302734\n",
      "30080 / 388463 4.27703857421875\n",
      "30240 / 388463 4.392393112182617\n",
      "30400 / 388463 3.9152579307556152\n",
      "30560 / 388463 3.900782823562622\n",
      "30720 / 388463 3.960710048675537\n",
      "30880 / 388463 3.8955554962158203\n",
      "31040 / 388463 3.9930286407470703\n",
      "31200 / 388463 3.9272022247314453\n",
      "31360 / 388463 4.27996301651001\n",
      "31520 / 388463 4.262862205505371\n",
      "31680 / 388463 3.743830919265747\n",
      "31840 / 388463 4.049739360809326\n",
      "32000 / 388463 4.236066818237305\n",
      "32160 / 388463 3.9526212215423584\n",
      "32320 / 388463 4.025532245635986\n",
      "32480 / 388463 3.8530566692352295\n",
      "32640 / 388463 4.279711723327637\n",
      "32800 / 388463 3.7527425289154053\n",
      "32960 / 388463 3.916304349899292\n",
      "33120 / 388463 3.9929721355438232\n",
      "33280 / 388463 4.0322442054748535\n",
      "33440 / 388463 3.9580705165863037\n",
      "33600 / 388463 4.158717632293701\n",
      "33760 / 388463 4.080479145050049\n",
      "33920 / 388463 4.06833028793335\n",
      "34080 / 388463 4.284471035003662\n",
      "34240 / 388463 4.018604755401611\n",
      "34400 / 388463 3.9329257011413574\n",
      "34560 / 388463 4.03041410446167\n",
      "34720 / 388463 3.9463236331939697\n",
      "34880 / 388463 3.6722893714904785\n",
      "35040 / 388463 3.9833438396453857\n",
      "35200 / 388463 4.032227993011475\n",
      "35360 / 388463 3.8651912212371826\n",
      "35520 / 388463 3.939697265625\n",
      "35680 / 388463 3.879833459854126\n",
      "35840 / 388463 4.000011444091797\n",
      "36000 / 388463 3.708064317703247\n",
      "36160 / 388463 4.0484700202941895\n",
      "36320 / 388463 4.043551445007324\n",
      "36480 / 388463 3.6064064502716064\n",
      "36640 / 388463 4.28206729888916\n",
      "36800 / 388463 4.029268741607666\n",
      "36960 / 388463 3.920576333999634\n",
      "37120 / 388463 4.378892421722412\n",
      "37280 / 388463 3.99900484085083\n",
      "37440 / 388463 4.054470539093018\n",
      "37600 / 388463 3.904726028442383\n",
      "37760 / 388463 4.0015549659729\n",
      "37920 / 388463 4.027009963989258\n",
      "38080 / 388463 3.8891632556915283\n",
      "38240 / 388463 3.9788382053375244\n",
      "38400 / 388463 4.066312313079834\n",
      "38560 / 388463 3.936030864715576\n",
      "38720 / 388463 3.9824867248535156\n",
      "38880 / 388463 4.017794132232666\n",
      "39040 / 388463 4.157229423522949\n",
      "39200 / 388463 3.7853164672851562\n",
      "39360 / 388463 4.0259809494018555\n",
      "39520 / 388463 4.521018028259277\n",
      "39680 / 388463 4.178747177124023\n",
      "39840 / 388463 4.118762016296387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 / 388463 3.9563515186309814\n",
      "40160 / 388463 3.761293888092041\n",
      "40320 / 388463 3.8995509147644043\n",
      "40480 / 388463 3.8591508865356445\n",
      "40640 / 388463 3.8854258060455322\n",
      "40800 / 388463 3.8320443630218506\n",
      "40960 / 388463 4.006970405578613\n",
      "41120 / 388463 3.93361234664917\n",
      "41280 / 388463 3.8793907165527344\n",
      "41440 / 388463 4.283437728881836\n",
      "41600 / 388463 4.03537654876709\n",
      "41760 / 388463 3.980067253112793\n",
      "41920 / 388463 3.762220621109009\n",
      "42080 / 388463 4.037419319152832\n",
      "42240 / 388463 4.070676803588867\n",
      "42400 / 388463 4.0283708572387695\n",
      "42560 / 388463 4.133161544799805\n",
      "42720 / 388463 3.9122753143310547\n",
      "42880 / 388463 3.827867269515991\n",
      "43040 / 388463 4.047313690185547\n",
      "43200 / 388463 3.9324216842651367\n",
      "43360 / 388463 3.9516348838806152\n",
      "43520 / 388463 4.106645584106445\n",
      "43680 / 388463 3.8975443840026855\n",
      "43840 / 388463 3.9248857498168945\n",
      "44000 / 388463 4.173266887664795\n",
      "44160 / 388463 4.034648895263672\n",
      "44320 / 388463 4.157316207885742\n",
      "44480 / 388463 3.89713454246521\n",
      "44640 / 388463 3.691746711730957\n",
      "44800 / 388463 4.038881778717041\n",
      "44960 / 388463 3.8474013805389404\n",
      "45120 / 388463 3.5496931076049805\n",
      "45280 / 388463 3.949204683303833\n",
      "45440 / 388463 3.801353693008423\n",
      "45600 / 388463 3.8136088848114014\n",
      "45760 / 388463 3.893038749694824\n",
      "45920 / 388463 3.985656261444092\n",
      "46080 / 388463 3.848349094390869\n",
      "46240 / 388463 3.923267364501953\n",
      "46400 / 388463 4.268482685089111\n",
      "46560 / 388463 4.045542240142822\n",
      "46720 / 388463 4.099599838256836\n",
      "46880 / 388463 3.640194892883301\n",
      "47040 / 388463 3.7653987407684326\n",
      "47200 / 388463 4.179837226867676\n",
      "47360 / 388463 3.8212006092071533\n",
      "47520 / 388463 4.143607139587402\n",
      "47680 / 388463 3.643281936645508\n",
      "47840 / 388463 3.673693895339966\n",
      "48000 / 388463 3.7600901126861572\n",
      "48160 / 388463 4.083307266235352\n",
      "48320 / 388463 4.05161714553833\n",
      "48480 / 388463 3.796583890914917\n",
      "48640 / 388463 3.754225254058838\n",
      "48800 / 388463 4.339824676513672\n",
      "48960 / 388463 3.7851011753082275\n",
      "49120 / 388463 4.13921594619751\n",
      "49280 / 388463 3.7259984016418457\n",
      "49440 / 388463 3.8395133018493652\n",
      "49600 / 388463 4.007169246673584\n",
      "49760 / 388463 4.119309425354004\n",
      "49920 / 388463 3.958138942718506\n",
      "50080 / 388463 3.7643303871154785\n",
      "50240 / 388463 3.8041906356811523\n",
      "50400 / 388463 4.002528190612793\n",
      "50560 / 388463 4.014713287353516\n",
      "50720 / 388463 3.815305709838867\n",
      "50880 / 388463 3.91926646232605\n",
      "51040 / 388463 3.818129301071167\n",
      "51200 / 388463 3.9443581104278564\n",
      "51360 / 388463 3.9726922512054443\n",
      "51520 / 388463 3.9392027854919434\n",
      "51680 / 388463 4.0709404945373535\n",
      "51840 / 388463 3.768120288848877\n",
      "52000 / 388463 4.133788108825684\n",
      "52160 / 388463 3.780700206756592\n",
      "52320 / 388463 3.813265323638916\n",
      "52480 / 388463 3.927650213241577\n",
      "52640 / 388463 3.991748571395874\n",
      "52800 / 388463 3.8668971061706543\n",
      "52960 / 388463 3.9146084785461426\n",
      "53120 / 388463 3.9862618446350098\n",
      "53280 / 388463 3.7799627780914307\n",
      "53440 / 388463 3.951594352722168\n",
      "53600 / 388463 3.730747699737549\n",
      "53760 / 388463 3.7693727016448975\n",
      "53920 / 388463 4.204463481903076\n",
      "54080 / 388463 4.0463547706604\n",
      "54240 / 388463 3.743757486343384\n",
      "54400 / 388463 3.802150011062622\n",
      "54560 / 388463 3.8953680992126465\n",
      "54720 / 388463 3.810274124145508\n",
      "54880 / 388463 3.9447550773620605\n",
      "55040 / 388463 3.9626996517181396\n",
      "55200 / 388463 4.082627773284912\n",
      "55360 / 388463 3.8540289402008057\n",
      "55520 / 388463 3.9143195152282715\n",
      "55680 / 388463 3.7300989627838135\n",
      "55840 / 388463 3.9747345447540283\n",
      "56000 / 388463 3.739563465118408\n",
      "56160 / 388463 3.6774821281433105\n",
      "56320 / 388463 3.9098925590515137\n",
      "56480 / 388463 3.883166551589966\n",
      "56640 / 388463 3.928520679473877\n",
      "56800 / 388463 3.995373249053955\n",
      "56960 / 388463 3.918670654296875\n",
      "57120 / 388463 4.134622097015381\n",
      "57280 / 388463 3.9230451583862305\n",
      "57440 / 388463 3.852849006652832\n",
      "57600 / 388463 3.875473737716675\n",
      "57760 / 388463 3.887883424758911\n",
      "57920 / 388463 3.624197244644165\n",
      "58080 / 388463 3.933643102645874\n",
      "58240 / 388463 3.746234178543091\n",
      "58400 / 388463 4.270639419555664\n",
      "58560 / 388463 4.003744602203369\n",
      "58720 / 388463 3.81886887550354\n",
      "58880 / 388463 3.9095592498779297\n",
      "59040 / 388463 3.697822332382202\n",
      "59200 / 388463 4.167954921722412\n",
      "59360 / 388463 3.69657039642334\n",
      "59520 / 388463 3.7669734954833984\n",
      "59680 / 388463 3.790074110031128\n",
      "59840 / 388463 3.8261427879333496\n",
      "60000 / 388463 3.870638608932495\n",
      "60160 / 388463 3.841614246368408\n",
      "60320 / 388463 4.086279392242432\n",
      "60480 / 388463 3.8056399822235107\n",
      "60640 / 388463 3.7101643085479736\n",
      "60800 / 388463 3.659395933151245\n",
      "60960 / 388463 3.553530216217041\n",
      "61120 / 388463 3.841510772705078\n",
      "61280 / 388463 3.961115837097168\n",
      "61440 / 388463 3.7795169353485107\n",
      "61600 / 388463 3.7166051864624023\n",
      "61760 / 388463 3.7881953716278076\n",
      "61920 / 388463 3.889523983001709\n",
      "62080 / 388463 3.607668161392212\n",
      "62240 / 388463 3.8555870056152344\n",
      "62400 / 388463 3.7379870414733887\n",
      "62560 / 388463 3.5097525119781494\n",
      "62720 / 388463 4.11386251449585\n",
      "62880 / 388463 3.4275877475738525\n",
      "63040 / 388463 4.057018280029297\n",
      "63200 / 388463 3.6923491954803467\n",
      "63360 / 388463 3.8040354251861572\n",
      "63520 / 388463 3.84541392326355\n",
      "63680 / 388463 3.658238649368286\n",
      "63840 / 388463 3.269965648651123\n",
      "64000 / 388463 3.8661773204803467\n",
      "64160 / 388463 4.146519184112549\n",
      "64320 / 388463 3.77917218208313\n",
      "64480 / 388463 3.8415908813476562\n",
      "64640 / 388463 3.8026554584503174\n",
      "64800 / 388463 3.854710102081299\n",
      "64960 / 388463 3.899233341217041\n",
      "65120 / 388463 3.745427370071411\n",
      "65280 / 388463 3.8039710521698\n",
      "65440 / 388463 3.5933053493499756\n",
      "65600 / 388463 4.241535186767578\n",
      "65760 / 388463 4.284351825714111\n",
      "65920 / 388463 4.090332508087158\n",
      "66080 / 388463 3.9004971981048584\n",
      "66240 / 388463 3.8966095447540283\n",
      "66400 / 388463 3.8031630516052246\n",
      "66560 / 388463 3.810633420944214\n",
      "66720 / 388463 3.594756603240967\n",
      "66880 / 388463 3.703111410140991\n",
      "67040 / 388463 4.022904396057129\n",
      "67200 / 388463 3.9247071743011475\n",
      "67360 / 388463 3.989672899246216\n",
      "67520 / 388463 3.6095733642578125\n",
      "67680 / 388463 3.6393492221832275\n",
      "67840 / 388463 3.8164191246032715\n",
      "68000 / 388463 4.120838642120361\n",
      "68160 / 388463 4.067511081695557\n",
      "68320 / 388463 3.5640180110931396\n",
      "68480 / 388463 3.9682438373565674\n",
      "68640 / 388463 4.14344596862793\n",
      "68800 / 388463 3.699885368347168\n",
      "68960 / 388463 3.9455621242523193\n",
      "69120 / 388463 3.949733257293701\n",
      "69280 / 388463 3.6312174797058105\n",
      "69440 / 388463 3.898159980773926\n",
      "69600 / 388463 3.9058728218078613\n",
      "69760 / 388463 3.9282665252685547\n",
      "69920 / 388463 3.7381527423858643\n",
      "70080 / 388463 4.0396223068237305\n",
      "70240 / 388463 3.6450042724609375\n",
      "70400 / 388463 3.876884698867798\n",
      "70560 / 388463 4.075435161590576\n",
      "70720 / 388463 3.659426212310791\n",
      "70880 / 388463 3.820769786834717\n",
      "71040 / 388463 3.8960886001586914\n",
      "71200 / 388463 3.861121416091919\n",
      "71360 / 388463 3.8169384002685547\n",
      "71520 / 388463 3.8732645511627197\n",
      "71680 / 388463 3.656357765197754\n",
      "71840 / 388463 3.5261802673339844\n",
      "72000 / 388463 3.7631266117095947\n",
      "72160 / 388463 4.093307018280029\n",
      "72320 / 388463 3.666869878768921\n",
      "72480 / 388463 3.6392605304718018\n",
      "72640 / 388463 3.759260416030884\n",
      "72800 / 388463 3.8932971954345703\n",
      "72960 / 388463 3.8721632957458496\n",
      "73120 / 388463 3.6998517513275146\n",
      "73280 / 388463 3.861116886138916\n",
      "73440 / 388463 3.605156421661377\n",
      "73600 / 388463 3.618180751800537\n",
      "73760 / 388463 3.750113010406494\n",
      "73920 / 388463 3.959376573562622\n",
      "74080 / 388463 3.5852537155151367\n",
      "74240 / 388463 3.7355246543884277\n",
      "74400 / 388463 3.693910598754883\n",
      "74560 / 388463 4.07304573059082\n",
      "74720 / 388463 4.096843242645264\n",
      "74880 / 388463 3.7487969398498535\n",
      "75040 / 388463 3.8673365116119385\n",
      "75200 / 388463 3.8543403148651123\n",
      "75360 / 388463 3.810361385345459\n",
      "75520 / 388463 3.7008893489837646\n",
      "75680 / 388463 3.6529903411865234\n",
      "75840 / 388463 3.908198833465576\n",
      "76000 / 388463 4.050784587860107\n",
      "76160 / 388463 3.6226727962493896\n",
      "76320 / 388463 3.756234645843506\n",
      "76480 / 388463 3.6272404193878174\n",
      "76640 / 388463 4.190269947052002\n",
      "76800 / 388463 3.8483388423919678\n",
      "76960 / 388463 3.609013319015503\n",
      "77120 / 388463 4.181662082672119\n",
      "77280 / 388463 3.658890962600708\n",
      "77440 / 388463 3.873046875\n",
      "77600 / 388463 3.7146224975585938\n",
      "77760 / 388463 3.732088088989258\n",
      "77920 / 388463 3.715240955352783\n",
      "78080 / 388463 3.785348892211914\n",
      "78240 / 388463 3.2680745124816895\n",
      "78400 / 388463 3.756286144256592\n",
      "78560 / 388463 3.958462715148926\n",
      "78720 / 388463 3.952164888381958\n",
      "78880 / 388463 3.8887743949890137\n",
      "79040 / 388463 3.867521286010742\n",
      "79200 / 388463 3.7072880268096924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79360 / 388463 3.74092435836792\n",
      "79520 / 388463 3.594423532485962\n",
      "79680 / 388463 3.679572820663452\n",
      "79840 / 388463 3.6389169692993164\n",
      "80000 / 388463 3.9129629135131836\n",
      "80160 / 388463 3.9419045448303223\n",
      "80320 / 388463 3.7117855548858643\n",
      "80480 / 388463 3.8782622814178467\n",
      "80640 / 388463 3.7391903400421143\n",
      "80800 / 388463 3.753458261489868\n",
      "80960 / 388463 3.964487075805664\n",
      "81120 / 388463 3.7300310134887695\n",
      "81280 / 388463 3.3936073780059814\n",
      "81440 / 388463 3.879814624786377\n",
      "81600 / 388463 3.8664603233337402\n",
      "81760 / 388463 3.707324981689453\n",
      "81920 / 388463 3.850999116897583\n",
      "82080 / 388463 3.7165029048919678\n",
      "82240 / 388463 3.6656644344329834\n",
      "82400 / 388463 3.499633312225342\n",
      "82560 / 388463 3.9974405765533447\n",
      "82720 / 388463 3.554377555847168\n",
      "82880 / 388463 3.9032604694366455\n",
      "83040 / 388463 4.040353298187256\n",
      "83200 / 388463 3.9348411560058594\n",
      "83360 / 388463 3.948547601699829\n",
      "83520 / 388463 3.8359899520874023\n",
      "83680 / 388463 3.7837493419647217\n",
      "83840 / 388463 3.6949737071990967\n",
      "84000 / 388463 3.725399971008301\n",
      "84160 / 388463 3.809377431869507\n",
      "84320 / 388463 3.921694040298462\n",
      "84480 / 388463 3.8452188968658447\n",
      "84640 / 388463 3.556718587875366\n",
      "84800 / 388463 3.7843730449676514\n",
      "84960 / 388463 3.4284725189208984\n",
      "85120 / 388463 3.5222320556640625\n",
      "85280 / 388463 4.020721435546875\n",
      "85440 / 388463 3.897059202194214\n",
      "85600 / 388463 4.0459818840026855\n",
      "85760 / 388463 3.9346654415130615\n",
      "85920 / 388463 3.762343406677246\n",
      "86080 / 388463 4.005344390869141\n",
      "86240 / 388463 3.6721484661102295\n",
      "86400 / 388463 3.7845630645751953\n",
      "86560 / 388463 3.890631914138794\n",
      "86720 / 388463 3.9765541553497314\n",
      "86880 / 388463 3.9220986366271973\n",
      "87040 / 388463 4.220832824707031\n",
      "87200 / 388463 3.572566509246826\n",
      "87360 / 388463 3.952824831008911\n",
      "87520 / 388463 3.4626107215881348\n",
      "87680 / 388463 3.779905080795288\n",
      "87840 / 388463 3.9068171977996826\n",
      "88000 / 388463 3.9423394203186035\n",
      "88160 / 388463 3.287919282913208\n",
      "88320 / 388463 3.9265060424804688\n",
      "88480 / 388463 3.8853209018707275\n",
      "88640 / 388463 3.710019111633301\n",
      "88800 / 388463 3.5574066638946533\n",
      "88960 / 388463 3.694258689880371\n",
      "89120 / 388463 3.6037967205047607\n",
      "89280 / 388463 3.6011056900024414\n",
      "89440 / 388463 4.006625175476074\n",
      "89600 / 388463 3.783827781677246\n",
      "89760 / 388463 3.7780983448028564\n",
      "89920 / 388463 3.672593355178833\n",
      "90080 / 388463 3.712482213973999\n",
      "90240 / 388463 3.8634040355682373\n",
      "90400 / 388463 3.755098581314087\n",
      "90560 / 388463 3.791764497756958\n",
      "90720 / 388463 3.9547317028045654\n",
      "90880 / 388463 3.5203475952148438\n",
      "91040 / 388463 3.7363603115081787\n",
      "91200 / 388463 3.818574905395508\n",
      "91360 / 388463 3.9575142860412598\n",
      "91520 / 388463 3.9883413314819336\n",
      "91680 / 388463 3.9052345752716064\n",
      "91840 / 388463 3.4502882957458496\n",
      "92000 / 388463 3.5448505878448486\n",
      "92160 / 388463 3.762397527694702\n",
      "92320 / 388463 3.8172032833099365\n",
      "92480 / 388463 3.740954875946045\n",
      "92640 / 388463 3.7624545097351074\n",
      "92800 / 388463 3.8069756031036377\n",
      "92960 / 388463 3.6688904762268066\n",
      "93120 / 388463 3.633037805557251\n",
      "93280 / 388463 3.8874261379241943\n",
      "93440 / 388463 3.8130686283111572\n",
      "93600 / 388463 3.2294161319732666\n",
      "93760 / 388463 3.8619465827941895\n",
      "93920 / 388463 3.5515496730804443\n",
      "94080 / 388463 3.715409278869629\n",
      "94240 / 388463 3.7765190601348877\n",
      "94400 / 388463 3.863538980484009\n",
      "94560 / 388463 3.603468418121338\n",
      "94720 / 388463 3.806185722351074\n",
      "94880 / 388463 3.538177013397217\n",
      "95040 / 388463 3.9640238285064697\n",
      "95200 / 388463 3.632091999053955\n",
      "95360 / 388463 3.471592426300049\n",
      "95520 / 388463 3.753401041030884\n",
      "95680 / 388463 3.5314407348632812\n",
      "95840 / 388463 3.7849323749542236\n",
      "96000 / 388463 3.897608995437622\n",
      "96160 / 388463 3.7841200828552246\n",
      "96320 / 388463 3.8067214488983154\n",
      "96480 / 388463 3.9064595699310303\n",
      "96640 / 388463 3.6451025009155273\n",
      "96800 / 388463 3.5530238151550293\n",
      "96960 / 388463 4.055022716522217\n",
      "97120 / 388463 3.9219446182250977\n",
      "97280 / 388463 3.9453084468841553\n",
      "97440 / 388463 3.8616740703582764\n",
      "97600 / 388463 4.170800685882568\n",
      "97760 / 388463 3.5705339908599854\n",
      "97920 / 388463 3.7145447731018066\n",
      "98080 / 388463 3.7803828716278076\n",
      "98240 / 388463 3.621537208557129\n",
      "98400 / 388463 3.829969644546509\n",
      "98560 / 388463 3.565232753753662\n",
      "98720 / 388463 3.5245161056518555\n",
      "98880 / 388463 3.796448230743408\n",
      "99040 / 388463 3.5711777210235596\n",
      "99200 / 388463 3.7870373725891113\n",
      "99360 / 388463 3.4961400032043457\n",
      "99520 / 388463 3.606752872467041\n",
      "99680 / 388463 3.592921733856201\n",
      "99840 / 388463 3.6302809715270996\n",
      "100000 / 388463 4.108436584472656\n",
      "100160 / 388463 3.4959936141967773\n",
      "100320 / 388463 3.7640883922576904\n",
      "100480 / 388463 3.803032398223877\n",
      "100640 / 388463 3.8774425983428955\n",
      "100800 / 388463 3.834940195083618\n",
      "100960 / 388463 3.906705141067505\n",
      "101120 / 388463 3.9763705730438232\n",
      "101280 / 388463 3.7013611793518066\n",
      "101440 / 388463 3.7397899627685547\n",
      "101600 / 388463 3.4865875244140625\n",
      "101760 / 388463 3.5758190155029297\n",
      "101920 / 388463 3.3520708084106445\n",
      "102080 / 388463 3.549217939376831\n",
      "102240 / 388463 3.672736167907715\n",
      "102400 / 388463 3.573261022567749\n",
      "102560 / 388463 3.7486236095428467\n",
      "102720 / 388463 3.866882801055908\n",
      "102880 / 388463 3.8517885208129883\n",
      "103040 / 388463 3.666893482208252\n",
      "103200 / 388463 3.628782033920288\n",
      "103360 / 388463 3.7910170555114746\n",
      "103520 / 388463 3.884010076522827\n",
      "103680 / 388463 3.9001078605651855\n",
      "103840 / 388463 3.8886406421661377\n",
      "104000 / 388463 3.7161009311676025\n",
      "104160 / 388463 3.8630025386810303\n",
      "104320 / 388463 3.5556998252868652\n",
      "104480 / 388463 3.696160316467285\n",
      "104640 / 388463 3.4799489974975586\n",
      "104800 / 388463 3.5578579902648926\n",
      "104960 / 388463 3.918179512023926\n",
      "105120 / 388463 3.8062310218811035\n",
      "105280 / 388463 3.987657308578491\n",
      "105440 / 388463 3.6739253997802734\n",
      "105600 / 388463 3.698720932006836\n",
      "105760 / 388463 3.8094379901885986\n",
      "105920 / 388463 3.8698179721832275\n",
      "106080 / 388463 4.081055164337158\n",
      "106240 / 388463 3.770352363586426\n",
      "106400 / 388463 3.891469717025757\n",
      "106560 / 388463 3.675138473510742\n",
      "106720 / 388463 3.638885259628296\n",
      "106880 / 388463 3.784404993057251\n",
      "107040 / 388463 3.682262897491455\n",
      "107200 / 388463 3.9344642162323\n",
      "107360 / 388463 3.7365071773529053\n",
      "107520 / 388463 3.7288708686828613\n",
      "107680 / 388463 3.820721387863159\n",
      "107840 / 388463 3.8375775814056396\n",
      "108000 / 388463 3.6105093955993652\n",
      "108160 / 388463 3.9819867610931396\n",
      "108320 / 388463 3.9545583724975586\n",
      "108480 / 388463 3.682241439819336\n",
      "108640 / 388463 3.917710065841675\n",
      "108800 / 388463 3.501582145690918\n",
      "108960 / 388463 3.6895015239715576\n",
      "109120 / 388463 3.578244686126709\n",
      "109280 / 388463 4.170418739318848\n",
      "109440 / 388463 3.6424190998077393\n",
      "109600 / 388463 3.879563331604004\n",
      "109760 / 388463 3.567345380783081\n",
      "109920 / 388463 3.611663341522217\n",
      "110080 / 388463 3.6588685512542725\n",
      "110240 / 388463 3.590319871902466\n",
      "110400 / 388463 3.8082427978515625\n",
      "110560 / 388463 3.8829243183135986\n",
      "110720 / 388463 3.6982948780059814\n",
      "110880 / 388463 3.8238911628723145\n",
      "111040 / 388463 3.667942762374878\n",
      "111200 / 388463 3.512233257293701\n",
      "111360 / 388463 3.688340663909912\n",
      "111520 / 388463 3.574589729309082\n",
      "111680 / 388463 3.922269105911255\n",
      "111840 / 388463 3.644146680831909\n",
      "112000 / 388463 3.7223031520843506\n",
      "112160 / 388463 3.790652275085449\n",
      "112320 / 388463 3.488936424255371\n",
      "112480 / 388463 3.7840614318847656\n",
      "112640 / 388463 4.131128787994385\n",
      "112800 / 388463 3.823235034942627\n",
      "112960 / 388463 3.6631932258605957\n",
      "113120 / 388463 3.525171995162964\n",
      "113280 / 388463 3.7303452491760254\n",
      "113440 / 388463 3.662571907043457\n",
      "113600 / 388463 3.4600555896759033\n",
      "113760 / 388463 3.8869218826293945\n",
      "113920 / 388463 4.030234336853027\n",
      "114080 / 388463 3.4908409118652344\n",
      "114240 / 388463 3.6413676738739014\n",
      "114400 / 388463 3.7060158252716064\n",
      "114560 / 388463 3.780512571334839\n",
      "114720 / 388463 3.758671760559082\n",
      "114880 / 388463 3.9013776779174805\n",
      "115040 / 388463 3.6443874835968018\n",
      "115200 / 388463 3.75771427154541\n",
      "115360 / 388463 3.795672655105591\n",
      "115520 / 388463 3.697368860244751\n",
      "115680 / 388463 3.4355645179748535\n",
      "115840 / 388463 3.693648099899292\n",
      "116000 / 388463 3.5802712440490723\n",
      "116160 / 388463 3.767430305480957\n",
      "116320 / 388463 3.6647284030914307\n",
      "116480 / 388463 3.844184637069702\n",
      "116640 / 388463 3.698498010635376\n",
      "116800 / 388463 3.7884528636932373\n",
      "116960 / 388463 3.843135118484497\n",
      "117120 / 388463 4.073597431182861\n",
      "117280 / 388463 3.8130178451538086\n",
      "117440 / 388463 3.813810110092163\n",
      "117600 / 388463 3.8395426273345947\n",
      "117760 / 388463 3.7433412075042725\n",
      "117920 / 388463 3.497501850128174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118080 / 388463 3.7235054969787598\n",
      "118240 / 388463 3.7670738697052\n",
      "118400 / 388463 3.6279985904693604\n",
      "118560 / 388463 3.7677102088928223\n",
      "118720 / 388463 3.623511552810669\n",
      "118880 / 388463 3.8234570026397705\n",
      "119040 / 388463 3.5134360790252686\n",
      "119200 / 388463 3.9765677452087402\n",
      "119360 / 388463 3.8477282524108887\n",
      "119520 / 388463 3.801976203918457\n",
      "119680 / 388463 3.465766191482544\n",
      "119840 / 388463 3.7235300540924072\n",
      "120000 / 388463 3.9122588634490967\n",
      "120160 / 388463 3.6804916858673096\n",
      "120320 / 388463 3.3895761966705322\n",
      "120480 / 388463 3.529493570327759\n",
      "120640 / 388463 3.7490029335021973\n",
      "120800 / 388463 3.308763027191162\n",
      "120960 / 388463 3.702219009399414\n",
      "121120 / 388463 3.782839775085449\n",
      "121280 / 388463 3.4591546058654785\n",
      "121440 / 388463 3.393665075302124\n",
      "121600 / 388463 3.8922150135040283\n",
      "121760 / 388463 3.7022323608398438\n",
      "121920 / 388463 3.5143723487854004\n",
      "122080 / 388463 3.9062671661376953\n",
      "122240 / 388463 3.6388347148895264\n",
      "122400 / 388463 3.5659732818603516\n",
      "122560 / 388463 3.602299928665161\n",
      "122720 / 388463 3.8773386478424072\n",
      "122880 / 388463 3.6045432090759277\n",
      "123040 / 388463 3.7698614597320557\n",
      "123200 / 388463 3.4962689876556396\n",
      "123360 / 388463 3.835031747817993\n",
      "123520 / 388463 3.659801721572876\n",
      "123680 / 388463 3.927476406097412\n",
      "123840 / 388463 3.9345760345458984\n",
      "124000 / 388463 3.8350281715393066\n",
      "124160 / 388463 3.8963756561279297\n",
      "124320 / 388463 3.897127628326416\n",
      "124480 / 388463 3.412431478500366\n",
      "124640 / 388463 3.415367841720581\n",
      "124800 / 388463 3.2897422313690186\n",
      "124960 / 388463 3.607367515563965\n",
      "125120 / 388463 3.5970380306243896\n",
      "125280 / 388463 3.4534387588500977\n",
      "125440 / 388463 3.679910659790039\n",
      "125600 / 388463 3.5589663982391357\n",
      "125760 / 388463 3.54677677154541\n",
      "125920 / 388463 3.813401222229004\n",
      "126080 / 388463 3.5823795795440674\n",
      "126240 / 388463 3.316040277481079\n",
      "126400 / 388463 3.7878849506378174\n",
      "126560 / 388463 3.5111258029937744\n",
      "126720 / 388463 3.607170820236206\n",
      "126880 / 388463 3.7898342609405518\n",
      "127040 / 388463 3.919151782989502\n",
      "127200 / 388463 3.6831445693969727\n",
      "127360 / 388463 3.8970084190368652\n",
      "127520 / 388463 3.439913511276245\n",
      "127680 / 388463 3.5633645057678223\n",
      "127840 / 388463 3.8520519733428955\n",
      "128000 / 388463 3.383106231689453\n",
      "128160 / 388463 3.563143014907837\n",
      "128320 / 388463 3.837294101715088\n",
      "128480 / 388463 3.7534561157226562\n",
      "128640 / 388463 3.6878974437713623\n",
      "128800 / 388463 3.7223575115203857\n",
      "128960 / 388463 3.4873950481414795\n",
      "129120 / 388463 3.7517290115356445\n",
      "129280 / 388463 3.740598678588867\n",
      "129440 / 388463 3.899883985519409\n",
      "129600 / 388463 3.8527116775512695\n",
      "129760 / 388463 3.7016408443450928\n",
      "129920 / 388463 3.504657745361328\n",
      "130080 / 388463 3.7196052074432373\n",
      "130240 / 388463 3.830735206604004\n",
      "130400 / 388463 3.8047990798950195\n",
      "130560 / 388463 3.587149143218994\n",
      "130720 / 388463 3.753798484802246\n",
      "130880 / 388463 3.397263526916504\n",
      "131040 / 388463 3.9093403816223145\n",
      "131200 / 388463 3.6976468563079834\n",
      "131360 / 388463 3.830876588821411\n",
      "131520 / 388463 3.6983590126037598\n",
      "131680 / 388463 3.8845272064208984\n",
      "131840 / 388463 3.4974584579467773\n",
      "132000 / 388463 3.6152801513671875\n",
      "132160 / 388463 3.6639065742492676\n",
      "132320 / 388463 3.560267448425293\n",
      "132480 / 388463 3.498162269592285\n",
      "132640 / 388463 3.8963513374328613\n",
      "132800 / 388463 3.7347497940063477\n",
      "132960 / 388463 3.8881585597991943\n",
      "133120 / 388463 3.3804728984832764\n",
      "133280 / 388463 3.7173874378204346\n",
      "133440 / 388463 3.4669454097747803\n",
      "133600 / 388463 3.5938432216644287\n",
      "133760 / 388463 3.855473041534424\n",
      "133920 / 388463 3.854288101196289\n",
      "134080 / 388463 3.595851182937622\n",
      "134240 / 388463 3.7728378772735596\n",
      "134400 / 388463 3.618661642074585\n",
      "134560 / 388463 3.577970027923584\n",
      "134720 / 388463 3.805600166320801\n",
      "134880 / 388463 3.2335753440856934\n",
      "135040 / 388463 3.5421268939971924\n",
      "135200 / 388463 3.708723783493042\n",
      "135360 / 388463 3.701350212097168\n",
      "135520 / 388463 3.780900001525879\n",
      "135680 / 388463 3.613907814025879\n",
      "135840 / 388463 3.8731439113616943\n",
      "136000 / 388463 3.5494260787963867\n",
      "136160 / 388463 3.8313026428222656\n",
      "136320 / 388463 3.715855836868286\n",
      "136480 / 388463 3.4650182723999023\n",
      "136640 / 388463 3.7329909801483154\n",
      "136800 / 388463 3.557905673980713\n",
      "136960 / 388463 3.280247688293457\n",
      "137120 / 388463 3.855430841445923\n",
      "137280 / 388463 3.72145938873291\n",
      "137440 / 388463 3.932971239089966\n",
      "137600 / 388463 3.7140746116638184\n",
      "137760 / 388463 3.8147525787353516\n",
      "137920 / 388463 3.4924933910369873\n",
      "138080 / 388463 3.403714179992676\n",
      "138240 / 388463 3.728015661239624\n",
      "138400 / 388463 3.4738643169403076\n",
      "138560 / 388463 3.367720365524292\n",
      "138720 / 388463 3.3298943042755127\n",
      "138880 / 388463 3.9506895542144775\n",
      "139040 / 388463 3.8437142372131348\n",
      "139200 / 388463 3.550117015838623\n",
      "139360 / 388463 3.444448232650757\n",
      "139520 / 388463 3.734027624130249\n",
      "139680 / 388463 3.554400682449341\n",
      "139840 / 388463 3.693521738052368\n",
      "140000 / 388463 3.486053943634033\n",
      "140160 / 388463 3.66483736038208\n",
      "140320 / 388463 3.695960521697998\n",
      "140480 / 388463 4.026170253753662\n",
      "140640 / 388463 3.939906358718872\n",
      "140800 / 388463 3.737513303756714\n",
      "140960 / 388463 3.6334948539733887\n",
      "141120 / 388463 3.9387195110321045\n",
      "141280 / 388463 3.9579484462738037\n",
      "141440 / 388463 3.756472587585449\n",
      "141600 / 388463 3.771300792694092\n",
      "141760 / 388463 3.401637315750122\n",
      "141920 / 388463 3.5068209171295166\n",
      "142080 / 388463 3.762272834777832\n",
      "142240 / 388463 3.778627872467041\n",
      "142400 / 388463 3.633366346359253\n",
      "142560 / 388463 3.5838119983673096\n",
      "142720 / 388463 3.7545292377471924\n",
      "142880 / 388463 3.54903507232666\n",
      "143040 / 388463 3.460744619369507\n",
      "143200 / 388463 3.696615695953369\n",
      "143360 / 388463 3.7648544311523438\n",
      "143520 / 388463 3.2789933681488037\n",
      "143680 / 388463 3.3722774982452393\n",
      "143840 / 388463 3.7426414489746094\n",
      "144000 / 388463 3.845445394515991\n",
      "144160 / 388463 3.772826671600342\n",
      "144320 / 388463 3.7726781368255615\n",
      "144480 / 388463 3.6561777591705322\n",
      "144640 / 388463 3.830711841583252\n",
      "144800 / 388463 3.806619882583618\n",
      "144960 / 388463 3.8449854850769043\n",
      "145120 / 388463 3.3977620601654053\n",
      "145280 / 388463 3.402229070663452\n",
      "145440 / 388463 3.7513279914855957\n",
      "145600 / 388463 3.591045379638672\n",
      "145760 / 388463 3.479994058609009\n",
      "145920 / 388463 4.033812522888184\n",
      "146080 / 388463 3.670032024383545\n",
      "146240 / 388463 3.4669511318206787\n",
      "146400 / 388463 3.3807148933410645\n",
      "146560 / 388463 3.6239962577819824\n",
      "146720 / 388463 3.3943898677825928\n",
      "146880 / 388463 3.655268669128418\n",
      "147040 / 388463 3.4967052936553955\n",
      "147200 / 388463 3.5277464389801025\n",
      "147360 / 388463 3.6205687522888184\n",
      "147520 / 388463 3.70851731300354\n",
      "147680 / 388463 3.7792043685913086\n",
      "147840 / 388463 3.49271821975708\n",
      "148000 / 388463 3.8463807106018066\n",
      "148160 / 388463 3.6372063159942627\n",
      "148320 / 388463 3.647311210632324\n",
      "148480 / 388463 3.6531989574432373\n",
      "148640 / 388463 3.330404758453369\n",
      "148800 / 388463 3.732841968536377\n",
      "148960 / 388463 3.6916847229003906\n",
      "149120 / 388463 3.447417736053467\n",
      "149280 / 388463 3.7654831409454346\n",
      "149440 / 388463 3.6390297412872314\n",
      "149600 / 388463 3.6540887355804443\n",
      "149760 / 388463 3.5657782554626465\n",
      "149920 / 388463 3.5943405628204346\n",
      "150080 / 388463 3.8343002796173096\n",
      "150240 / 388463 3.3362696170806885\n",
      "150400 / 388463 3.467005491256714\n",
      "150560 / 388463 3.6689505577087402\n",
      "150720 / 388463 3.2537808418273926\n",
      "150880 / 388463 3.7625880241394043\n",
      "151040 / 388463 3.4482638835906982\n",
      "151200 / 388463 3.6258668899536133\n",
      "151360 / 388463 3.8445286750793457\n",
      "151520 / 388463 3.2909724712371826\n",
      "151680 / 388463 3.7883172035217285\n",
      "151840 / 388463 3.376849412918091\n",
      "152000 / 388463 3.6717045307159424\n",
      "152160 / 388463 3.3794732093811035\n",
      "152320 / 388463 3.6453263759613037\n",
      "152480 / 388463 3.372073173522949\n",
      "152640 / 388463 3.2416141033172607\n",
      "152800 / 388463 3.652311086654663\n",
      "152960 / 388463 3.5471067428588867\n",
      "153120 / 388463 3.6996002197265625\n",
      "153280 / 388463 3.6141695976257324\n",
      "153440 / 388463 3.7800302505493164\n",
      "153600 / 388463 3.909250020980835\n",
      "153760 / 388463 3.5716466903686523\n",
      "153920 / 388463 3.6794495582580566\n",
      "154080 / 388463 3.5868802070617676\n",
      "154240 / 388463 3.436126708984375\n",
      "154400 / 388463 3.4379818439483643\n",
      "154560 / 388463 3.550206184387207\n",
      "154720 / 388463 3.5819780826568604\n",
      "154880 / 388463 3.8207924365997314\n",
      "155040 / 388463 3.753364324569702\n",
      "155200 / 388463 3.513458728790283\n",
      "155360 / 388463 3.419607400894165\n",
      "155520 / 388463 3.7626659870147705\n",
      "155680 / 388463 3.832024097442627\n",
      "155840 / 388463 3.415910243988037\n",
      "156000 / 388463 3.599533796310425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156160 / 388463 3.9078996181488037\n",
      "156320 / 388463 3.575254440307617\n",
      "156480 / 388463 3.701608657836914\n",
      "156640 / 388463 4.009967803955078\n",
      "156800 / 388463 3.5274055004119873\n",
      "156960 / 388463 3.7892720699310303\n",
      "157120 / 388463 3.7613067626953125\n",
      "157280 / 388463 3.803645133972168\n",
      "157440 / 388463 3.8468735218048096\n",
      "157600 / 388463 3.744687080383301\n",
      "157760 / 388463 3.8340342044830322\n",
      "157920 / 388463 3.77496600151062\n",
      "158080 / 388463 3.990816116333008\n",
      "158240 / 388463 3.353952646255493\n",
      "158400 / 388463 3.6630303859710693\n",
      "158560 / 388463 3.5656909942626953\n",
      "158720 / 388463 3.670877456665039\n",
      "158880 / 388463 3.753894805908203\n",
      "159040 / 388463 3.6330127716064453\n",
      "159200 / 388463 3.734480142593384\n",
      "159360 / 388463 3.8219985961914062\n",
      "159520 / 388463 3.5899386405944824\n",
      "159680 / 388463 3.6933023929595947\n",
      "159840 / 388463 3.6431260108947754\n",
      "160000 / 388463 3.4203460216522217\n",
      "160160 / 388463 3.7840206623077393\n",
      "160320 / 388463 3.8010413646698\n",
      "160480 / 388463 3.4566822052001953\n",
      "160640 / 388463 3.74344539642334\n",
      "160800 / 388463 3.4532806873321533\n",
      "160960 / 388463 3.493501901626587\n",
      "161120 / 388463 3.5956084728240967\n",
      "161280 / 388463 3.5929977893829346\n",
      "161440 / 388463 3.895498752593994\n",
      "161600 / 388463 3.593883991241455\n",
      "161760 / 388463 3.4108338356018066\n",
      "161920 / 388463 3.6499741077423096\n",
      "162080 / 388463 3.765596628189087\n",
      "162240 / 388463 3.6106951236724854\n",
      "162400 / 388463 3.32570743560791\n",
      "162560 / 388463 3.942406415939331\n",
      "162720 / 388463 3.5638551712036133\n",
      "162880 / 388463 3.461792230606079\n",
      "163040 / 388463 3.6491169929504395\n",
      "163200 / 388463 3.7490813732147217\n",
      "163360 / 388463 3.679497480392456\n",
      "163520 / 388463 3.6024200916290283\n",
      "163680 / 388463 3.6509671211242676\n",
      "163840 / 388463 3.7119128704071045\n",
      "164000 / 388463 3.653505802154541\n",
      "164160 / 388463 3.7123453617095947\n",
      "164320 / 388463 3.8968708515167236\n",
      "164480 / 388463 3.996532440185547\n",
      "164640 / 388463 3.4841079711914062\n",
      "164800 / 388463 3.3563547134399414\n",
      "164960 / 388463 3.430997848510742\n",
      "165120 / 388463 3.4225358963012695\n",
      "165280 / 388463 3.6097683906555176\n",
      "165440 / 388463 3.61919903755188\n",
      "165600 / 388463 3.645019054412842\n",
      "165760 / 388463 3.4221596717834473\n",
      "165920 / 388463 3.894944667816162\n",
      "166080 / 388463 3.775482177734375\n",
      "166240 / 388463 3.439832925796509\n",
      "166400 / 388463 3.8028769493103027\n",
      "166560 / 388463 3.5299384593963623\n",
      "166720 / 388463 3.894451379776001\n",
      "166880 / 388463 3.7607903480529785\n",
      "167040 / 388463 3.6406142711639404\n",
      "167200 / 388463 3.4564483165740967\n",
      "167360 / 388463 3.6547136306762695\n",
      "167520 / 388463 3.8341476917266846\n",
      "167680 / 388463 3.679929256439209\n",
      "167840 / 388463 3.5737335681915283\n",
      "168000 / 388463 3.3912670612335205\n",
      "168160 / 388463 3.4698691368103027\n",
      "168320 / 388463 3.5297658443450928\n",
      "168480 / 388463 3.632251262664795\n",
      "168640 / 388463 3.5561439990997314\n",
      "168800 / 388463 3.697122573852539\n",
      "168960 / 388463 3.5746376514434814\n",
      "169120 / 388463 3.8669686317443848\n",
      "169280 / 388463 3.6019797325134277\n",
      "169440 / 388463 3.491420030593872\n",
      "169600 / 388463 3.406604528427124\n",
      "169760 / 388463 3.6532931327819824\n",
      "169920 / 388463 3.557145118713379\n",
      "170080 / 388463 3.4753024578094482\n",
      "170240 / 388463 3.6970250606536865\n",
      "170400 / 388463 3.5851118564605713\n",
      "170560 / 388463 3.6875784397125244\n",
      "170720 / 388463 3.559469699859619\n",
      "170880 / 388463 3.8566548824310303\n",
      "171040 / 388463 3.694631814956665\n",
      "171200 / 388463 3.6867921352386475\n",
      "171360 / 388463 3.585571527481079\n",
      "171520 / 388463 3.49910831451416\n",
      "171680 / 388463 3.8175199031829834\n",
      "171840 / 388463 3.7575886249542236\n",
      "172000 / 388463 3.5247554779052734\n",
      "172160 / 388463 3.6502060890197754\n",
      "172320 / 388463 3.507948875427246\n",
      "172480 / 388463 3.51786470413208\n",
      "172640 / 388463 3.66530704498291\n",
      "172800 / 388463 3.646054744720459\n",
      "172960 / 388463 3.323806047439575\n",
      "173120 / 388463 3.512134552001953\n",
      "173280 / 388463 3.630762815475464\n",
      "173440 / 388463 3.8669519424438477\n",
      "173600 / 388463 3.6955769062042236\n",
      "173760 / 388463 3.5366272926330566\n",
      "173920 / 388463 3.6028614044189453\n",
      "174080 / 388463 3.8332772254943848\n",
      "174240 / 388463 4.056838035583496\n",
      "174400 / 388463 3.6401641368865967\n",
      "174560 / 388463 3.69966721534729\n",
      "174720 / 388463 3.5990028381347656\n",
      "174880 / 388463 3.7194149494171143\n",
      "175040 / 388463 3.849367618560791\n",
      "175200 / 388463 3.7502646446228027\n",
      "175360 / 388463 3.5520131587982178\n",
      "175520 / 388463 3.6809136867523193\n",
      "175680 / 388463 3.724297285079956\n",
      "175840 / 388463 3.540009021759033\n",
      "176000 / 388463 3.5519938468933105\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 942.00 MiB (GPU 0; 4.00 GiB total capacity; 1.89 GiB already allocated; 643.61 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1984/480026119.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Random\\anaconda\\envs\\tii\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Random\\anaconda\\envs\\tii\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 942.00 MiB (GPU 0; 4.00 GiB total capacity; 1.89 GiB already allocated; 643.61 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for b in range(0, len(idx), batchSize):\n",
    "    batch = [ trainCorpus[i] for i in idx[b:min(b+batchSize, len(idx))] ]\n",
    "    H = lm(batch)\n",
    "    optimizer.zero_grad()\n",
    "    H.backward()\n",
    "    optimizer.step()\n",
    "    if b % 10 == 0:\n",
    "        print(b, '/', len(idx), H.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(lm, testCorpus, batchSize):\n",
    "    H = 0.\n",
    "    c = 0\n",
    "    for b in range(0,len(testCorpus),batchSize):\n",
    "        batch = testCorpus[b:min(b+batchSize, len(testCorpus))]\n",
    "        l = sum(len(s)-1 for s in batch)\n",
    "        c += l\n",
    "        with torch.no_grad():\n",
    "            H += l * lm(batch)\n",
    "    return math.exp(H/c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.608086011322"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(lm, testCorpus, batchSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Двупосочен LSTM с пакетиране на партида"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMLanguageModelPack(torch.nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, word2ind, unkToken, padToken, endToken):\n",
    "        super(BiLSTMLanguageModelPack, self).__init__()\n",
    "        self.word2ind = word2ind\n",
    "        self.unkTokenIdx = word2ind[unkToken]\n",
    "        self.padTokenIdx = word2ind[padToken]\n",
    "        self.endTokenIdx = word2ind[endToken]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = torch.nn.LSTM(embed_size, hidden_size, bidirectional=True)\n",
    "        self.embed = torch.nn.Embedding(len(word2ind), embed_size)\n",
    "        self.projection = torch.nn.Linear(2*hidden_size,len(word2ind))\n",
    "\n",
    "    def preparePaddedBatch(self, source): #\n",
    "        device = next(self.parameters()).device\n",
    "        m = max(len(s) for s in source)\n",
    "        sents = [[self.word2ind.get(w,self.unkTokenIdx) for w in s] for s in source]\n",
    "        sents_padded = [ s+(m-len(s))*[self.padTokenIdx] for s in sents]\n",
    "        return torch.t(torch.tensor(sents_padded, dtype=torch.long, device=device))\n",
    "\n",
    "    def forward(self, source):\n",
    "        batch_size = len(source)\n",
    "        X = self.preparePaddedBatch(source) # (w,s)\n",
    "        E = self.embed(X) # (w,s,e)\n",
    "        \n",
    "        source_lengths = [len(s) for s in source]\n",
    "        m = X.shape[0]\n",
    "        outputPacked, _ = self.lstm(torch.nn.utils.rnn.pack_padded_sequence(E, source_lengths,enforce_sorted=False)) #bd\n",
    "        \n",
    "        output,_ = torch.nn.utils.rnn.pad_packed_sequence(outputPacked) # (w,s,2h) # d\n",
    "        output = output.view(m, batch_size, 2, self.hidden_size) # (w,s,2,h) # d\n",
    "        t = torch.cat((output[:-2,:,0,:], output[2:,:,1,:]),2) # (w,s,2h) # d\n",
    "        Z = self.projection(t.flatten(0,1)) # (w*s,2h)\n",
    "\n",
    "        Y_bar = X[1:-1].flatten(0,1)\n",
    "        Y_bar[Y_bar==self.endTokenIdx] = self.padTokenIdx\n",
    "        H = torch.nn.functional.cross_entropy(Z,Y_bar,ignore_index=self.padTokenIdx)\n",
    "        return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "blm = BiLSTMLanguageModelPack(emb_size, hid_size, word2ind, unkToken, padToken, endToken).to(device)\n",
    "optimizer = torch.optim.Adam(blm.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(len(trainCorpus), dtype='int32')\n",
    "np.random.shuffle(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(0, len(idx), batchSize):\n",
    "    batch = [ trainCorpus[i] for i in idx[b:min(b+batchSize, len(idx))] ]\n",
    "    H = blm(batch)\n",
    "    optimizer.zero_grad()\n",
    "    H.backward()\n",
    "    optimizer.step()\n",
    "    if b % 10 == 0:\n",
    "        print(b, '/', len(idx), H.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(blm, testCorpus, batchSize):\n",
    "    H = 0.\n",
    "    c = 0\n",
    "    for b in range(0,len(testCorpus),batchSize):\n",
    "        batch = testCorpus[b:min(b+batchSize, len(testCorpus))]\n",
    "        l = sum(len(s)-2 for s in batch)\n",
    "        c += l\n",
    "        with torch.no_grad():\n",
    "            H += l * blm(batch)\n",
    "    return math.exp(H/c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.285752269193237"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(blm, testCorpus, batchSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LSTM класификатор на документи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(torch.nn.Module):\n",
    "    def __init__(self, langModel, classesCount):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.langModel = langModel\n",
    "        self.classProjection = torch.nn.Linear(langModel.lstm.hidden_size,classesCount)\n",
    "    \n",
    "    def forward(self, source):\n",
    "        X = self.langModel.preparePaddedBatch(source)\n",
    "        E = self.langModel.embed(X[:-1])\n",
    "        source_lengths = [len(s)-1 for s in source]\n",
    "        _, (h,_) = self.langModel.lstm(torch.nn.utils.rnn.pack_padded_sequence(E, source_lengths,enforce_sorted=False))\n",
    "        \n",
    "        Z = self.classProjection(torch.squeeze(h,dim=0)) # (s,h) -> (s,c)\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileNames = myCorpus.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoCorpus = [ [startToken] + [w.lower() for w in myCorpus.words(f)] + [endToken] for f in fileNames if f.find('E-Economy'+'/')==0 ]\n",
    "milCorpus = [ [startToken] + [w.lower() for w in myCorpus.words(f)] + [endToken] for f in fileNames if f.find('S-Military'+'/')==0 ]\n",
    "polCorpus = [ [startToken] + [w.lower() for w in myCorpus.words(f)] + [endToken] for f in fileNames if f.find('J-Politics'+'/')==0 ]\n",
    "culCorpus = [ [startToken] + [w.lower() for w in myCorpus.words(f)] + [endToken] for f in fileNames if f.find('C-Culture'+'/')==0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "testEcoCorpus, trainEcoCorpus = splitSentCorpus(ecoCorpus)\n",
    "testMilCorpus, trainMilCorpus = splitSentCorpus(milCorpus)\n",
    "testPolCorpus, trainPolCorpus = splitSentCorpus(polCorpus)\n",
    "testCulCorpus, trainCulCorpus = splitSentCorpus(culCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainClassCorpus = trainEcoCorpus + trainMilCorpus + trainPolCorpus + trainCulCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = np.concatenate((\n",
    "                         np.ones(len(trainEcoCorpus),dtype='int32')*0,\n",
    "                         np.ones(len(trainMilCorpus),dtype='int32')*1,\n",
    "                         np.ones(len(trainPolCorpus),dtype='int32')*2,\n",
    "                         np.ones(len(trainCulCorpus),dtype='int32')*3\n",
    "                         ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "testY = np.concatenate((\n",
    "                        np.ones(len(testEcoCorpus),dtype='int32')*0,\n",
    "                        np.ones(len(testMilCorpus),dtype='int32')*1,\n",
    "                        np.ones(len(testPolCorpus),dtype='int32')*2,\n",
    "                        np.ones(len(testCulCorpus),dtype='int32')*3\n",
    "                        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(len(trainClassCorpus), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "classModel = LSTMClassifier(lm,4).to(device)\n",
    "optimizer = torch.optim.Adam(classModel.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(idx)\n",
    "for b in range(0, len(idx), batchSize):\n",
    "    batch = [ trainClassCorpus[i] for i in idx[b:min(b+batchSize, len(idx))] ]\n",
    "    target = torch.tensor(trainY[idx[b:min(b+batchSize, len(idx))]], dtype = torch.long, device = device)\n",
    "\n",
    "    Z = classModel(batch)\n",
    "    H = torch.nn.functional.cross_entropy(Z,target)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    H.backward()\n",
    "    optimizer.step()\n",
    "    if b % 10 == 0:\n",
    "        print(b, '/', len(idx), H.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "testClassCorpus = [ testEcoCorpus, testMilCorpus, testPolCorpus, testCulCorpus ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma(s):\n",
    "    with torch.no_grad():\n",
    "        Z = classModel([s])\n",
    "        return torch.argmax(Z[0]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testClassifier(testClassCorpus, gamma):\n",
    "    L = [ len(c) for c in testClassCorpus ]\n",
    "    pb = progressBar(50)\n",
    "    pb.start(sum(L))\n",
    "    classesCount = len(testClassCorpus)\n",
    "    confusionMatrix = [ [0] * classesCount for _ in range(classesCount) ]\n",
    "    for c in range(classesCount):\n",
    "        for text in testClassCorpus[c]:\n",
    "            pb.tick()\n",
    "            c_MAP = gamma(text)\n",
    "            confusionMatrix[c][c_MAP] += 1\n",
    "    pb.stop()\n",
    "    precision = []\n",
    "    recall = []\n",
    "    Fscore = []\n",
    "    for c in range(classesCount):\n",
    "        extracted = sum(confusionMatrix[x][c] for x in range(classesCount))\n",
    "        if confusionMatrix[c][c] == 0:\n",
    "            precision.append(0.0)\n",
    "            recall.append(0.0)\n",
    "            Fscore.append(0.0)\n",
    "        else:\n",
    "            precision.append( confusionMatrix[c][c] / extracted )\n",
    "            recall.append( confusionMatrix[c][c] / L[c] )\n",
    "            Fscore.append((2.0 * precision[c] * recall[c]) / (precision[c] + recall[c]))\n",
    "    P = sum( L[c] * precision[c] / sum(L) for c in range(classesCount) )\n",
    "    R = sum( L[c] * recall[c] / sum(L) for c in range(classesCount) )\n",
    "    F1 = (2*P*R) / (P + R)\n",
    "    print('=================================================================')\n",
    "    print('Матрица на обърквания: ')\n",
    "    for row in confusionMatrix:\n",
    "        for val in row:\n",
    "            print('{:4}'.format(val), end = '')\n",
    "        print()\n",
    "    print('Прецизност: '+str(precision))\n",
    "    print('Обхват: '+str(recall))\n",
    "    print('F-оценка: '+str(Fscore))\n",
    "    print('Обща презизност: '+str(P))\n",
    "    print('Общ обхват: '+str(R))\n",
    "    print('Обща F-оценка: '+str(F1))\n",
    "    print('=================================================================')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                                  --------------------------------------------------]\n",
      "=================================================================\n",
      "Матрица на обърквания: \n",
      "   1  18  46   0\n",
      "   1 144  11   2\n",
      "   0  29 705   1\n",
      "   0   1   2  42\n",
      "Прецизност: [0.5, 0.75, 0.9227748691099477, 0.9333333333333333]\n",
      "Обхват: [0.015384615384615385, 0.9113924050632911, 0.9591836734693877, 0.9333333333333333]\n",
      "F-оценка: [0.029850746268656723, 0.8228571428571428, 0.9406270847231488, 0.9333333333333333]\n",
      "Обща презизност: 0.8686336279120754\n",
      "Общ обхват: 0.8893320039880359\n",
      "Обща F-оценка: 0.8788609640877645\n",
      "=================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testClassifier(testClassCorpus, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Двупосочен LSTM класификатор на документи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMClassifier(torch.nn.Module):\n",
    "    def __init__(self, langModel, classesCount):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.langModel = langModel\n",
    "        self.classProjection = torch.nn.Linear(2*langModel.hidden_size,classesCount)\n",
    "    \n",
    "    def forward(self, source):\n",
    "        batch_size = len(source)\n",
    "        X = self.langModel.preparePaddedBatch(source)\n",
    "        E = self.langModel.embed(X)\n",
    "        source_lengths = [len(s) for s in source]\n",
    "        _, (h,c) = self.langModel.lstm(torch.nn.utils.rnn.pack_padded_sequence(E, source_lengths,enforce_sorted=False))\n",
    "        h = h.view(2,batch_size,self.langModel.hidden_size)\n",
    "        \n",
    "        Z = self.classProjection(torch.cat([h[0],h[1]],1)) # (batch_size,2*self.langModel.hidden_size) -> (batch_size,4=|C|)\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "classModelB = BiLSTMClassifier(blm,4).to(device)\n",
    "optimizer = torch.optim.Adam(classModelB.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(len(trainClassCorpus), dtype='int32')\n",
    "np.random.shuffle(idx)\n",
    "for b in range(0, len(idx), batchSize):\n",
    "    batch = [ trainClassCorpus[i] for i in idx[b:min(b+batchSize, len(idx))] ]\n",
    "    target = torch.tensor(trainY[idx[b:min(b+batchSize, len(idx))]], dtype = torch.long, device = device)\n",
    "    \n",
    "    Z = classModelB(batch)\n",
    "    H = torch.nn.functional.cross_entropy(Z,target)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    H.backward()\n",
    "    optimizer.step()\n",
    "    if b % 10 == 0:\n",
    "        print(b, '/', len(idx), H.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma(s):\n",
    "    with torch.no_grad():\n",
    "        Z = classModelB([s])\n",
    "        return torch.argmax(Z[0]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                                  --------------------------------------------------]\n",
      "=================================================================\n",
      "Матрица на обърквания: \n",
      "  40   3  21   1\n",
      "  15 135   8   0\n",
      "   5   5 725   0\n",
      "   5   0   0  40\n",
      "Прецизност: [0.6153846153846154, 0.9440559440559441, 0.9615384615384616, 0.975609756097561]\n",
      "Обхват: [0.6153846153846154, 0.8544303797468354, 0.9863945578231292, 0.8888888888888888]\n",
      "F-оценка: [0.6153846153846154, 0.8970099667774087, 0.9738079247817326, 0.9302325581395349]\n",
      "Обща презизност: 0.9369830981216337\n",
      "Общ обхват: 0.9371884346959122\n",
      "Обща F-оценка: 0.9370857551603071\n",
      "=================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testClassifier(testClassCorpus, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Конволюционен класификатор на документи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionClassifier(torch.nn.Module):\n",
    "    def __init__(self, embed, filterSize, filterCount, classesCount, word2ind, unkToken, padToken):\n",
    "        super(ConvolutionClassifier, self).__init__()\n",
    "        self.embed = embed\n",
    "        self.word2ind = word2ind\n",
    "        self.unkTokenIdx = word2ind[unkToken]\n",
    "        self.padTokenIdx = word2ind[padToken]\n",
    "        self.convolution = torch.nn.Conv1d(in_channels=embed.embedding_dim, out_channels=filterCount, kernel_size=filterSize)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.classProjection = torch.nn.Linear(filterCount,classesCount)\n",
    "    \n",
    "    def preparePaddedBatch(self, source):\n",
    "        device = next(self.parameters()).device\n",
    "        m = max(len(s) for s in source)\n",
    "        sents = [[self.word2ind.get(w,self.unkTokenIdx) for w in s] for s in source]\n",
    "        sents_padded = [ s+(m-len(s))*[self.padTokenIdx] for s in sents]\n",
    "        return torch.tensor(sents_padded, dtype=torch.long, device=device) # (batch_size,max_sent_len,embed_size)\n",
    "    \n",
    "    def forward(self, source):\n",
    "        X = self.preparePaddedBatch(source)\n",
    "        \n",
    "        E = torch.transpose(self.embed(X),1,2) # (s,e,w)\n",
    "        ### Очаква се Е да е тензор с размер (batch_size, embed_size, max_sent_len)\n",
    "\n",
    "        U,_ = torch.max(torch.relu(self.convolution(E)), dim=2) # (s,oc,w) # d -> (s,oc)\n",
    "        Z = self.classProjection(self.dropout(U))\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB = lm.embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "classModelE = ConvolutionClassifier(EMB, 7, 400, 4, word2ind, unkToken, padToken).to(device)\n",
    "optimizer = torch.optim.Adam(classModelE.parameters(), lr=0.01, weight_decay=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(len(trainClassCorpus), dtype='int32')\n",
    "classModelE.train() # d\n",
    "for epoch in range(10):\n",
    "    np.random.shuffle(idx)\n",
    "    for b in range(0, len(idx), batchSize):\n",
    "        batch = [ trainClassCorpus[i] for i in idx[b:min(b+batchSize, len(idx))] ]\n",
    "        target = torch.tensor(trainY[idx[b:min(b+batchSize, len(idx))]], dtype = torch.long, device = device)\n",
    "    \n",
    "        Z = classModelE(batch)\n",
    "        H = torch.nn.functional.cross_entropy(Z,target)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        H.backward()\n",
    "        optimizer.step()\n",
    "        if b % 10 == 0:\n",
    "            print(b, '/', len(idx), H.item())\n",
    "classModelE.eval()\n",
    "testClassifier(testClassCorpus, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma(s):\n",
    "    with torch.no_grad():\n",
    "        Z = classModelE([s])\n",
    "        return torch.argmax(Z[0]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                                  --------------------------------------------------]\n",
      "=================================================================\n",
      "Матрица на обърквания: \n",
      "  45   0  20   0\n",
      "   0 146  12   0\n",
      "   0   3 732   0\n",
      "   0   0   2  43\n",
      "Прецизност: [1.0, 0.9798657718120806, 0.9556135770234987, 1.0]\n",
      "Обхват: [0.6923076923076923, 0.9240506329113924, 0.9959183673469387, 0.9555555555555556]\n",
      "F-оценка: [0.8181818181818181, 0.9511400651465798, 0.9753497668221185, 0.9772727272727273]\n",
      "Обща презизност: 0.9643018654621938\n",
      "Общ обхват: 0.9631106679960121\n",
      "Обща F-оценка: 0.9637058986316202\n",
      "=================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testClassifier(testClassCorpus, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LSTM с посимволово влагане с КНН и пакетиране на партида"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharEmbedding(torch.nn.Module):\n",
    "    def __init__(self, word2ind, char_embed_size, word_embed_size, filter_size=5, dropoutrate=0.3, padding=1):\n",
    "        super(CharEmbedding, self).__init__()\n",
    "        self.word2ind = word2ind\n",
    "        self.char_embed_size = char_embed_size\n",
    "        self.word_embed_size = word_embed_size\n",
    "        self.filter_size = filter_size\n",
    "        self.dropoutrate = dropoutrate\n",
    "        self.padding = padding\n",
    "\n",
    "        alphabetSet = {c for w in word2ind for c in w}\n",
    "        alphabet = ['§','`','~','№']+list(alphabetSet)\n",
    "        self.char2id = {c:i for i, c in enumerate(alphabet) }\n",
    "        self.char_pad = self.char2id['§']\n",
    "        self.start_of_word = self.char2id['`']\n",
    "        self.end_of_word = self.char2id['~']\n",
    "        self.char_unk = self.char2id['№']\n",
    "\n",
    "        self.CharEmbedding = torch.nn.Embedding(len(self.char2id),self.char_embed_size, padding_idx = self.char_pad)\n",
    "        self.conv = torch.nn.Conv1d(char_embed_size, word_embed_size, filter_size, padding=padding)\n",
    "        self.highway_proj = torch.nn.Linear(word_embed_size,word_embed_size)\n",
    "        self.highway_gate = torch.nn.Linear(word_embed_size,word_embed_size)\n",
    "\n",
    "        self.Dropout = torch.nn.Dropout(dropoutrate)\n",
    "\n",
    "    def preparePaddedBatch(self, source):\n",
    "        device = next(self.parameters()).device\n",
    "        source_ids = [[ [self.start_of_word] + [self.char2id.get(c, self.char_unk) for c in w ] + [self.end_of_word] for w in s] for s in source]\n",
    "\n",
    "        max_word_length = max(len(w) for s in source_ids for w in s )\n",
    "        max_sent_len = max(len(s) for s in source_ids)\n",
    "    \n",
    "        sents_padded = []\n",
    "        for sentence in source_ids:\n",
    "            sent_padded = [ w + [self.char_pad]*(max_word_length-len(w)) for w in sentence ] + [[self.char_pad]*max_word_length] * (max_sent_len - len(sentence))\n",
    "            sents_padded.append(sent_padded)\n",
    "\n",
    "        return torch.transpose(torch.tensor(sents_padded, dtype=torch.long, device=device),0,1).contiguous()\n",
    "\n",
    "    def forward(self, source):\n",
    "        batch_size = len(source) # (s,w,c)\n",
    "        X = self.preparePaddedBatch(source) # (w,s,c)\n",
    "        X_emb = self.CharEmbedding(X).transpose(2,3) # (w,s,ce,c)\n",
    "\n",
    "        x_conv = self.conv(X_emb.flatten(0,1)) # (w*s,we,c)\n",
    "        x_conv_out0,_ = torch.max(torch.nn.functional.relu(x_conv),dim=2) # (w*s,we)\n",
    "        x_conv_out = x_conv_out0.view((-1,batch_size,self.word_embed_size)) # (w,s,we)\n",
    "\n",
    "        x_proj = torch.nn.functional.relu(self.highway_proj(x_conv_out)) # (w,s,we)\n",
    "        x_gate = torch.sigmoid(self.highway_gate(x_conv_out)) # (w,s,we)\n",
    "        x_highway = x_gate * x_proj + (1 - x_gate) * x_conv_out # (w,s,we)\n",
    "\n",
    "        output = self.Dropout(x_highway)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharCNNLSTMLanguageModelPack(torch.nn.Module):\n",
    "    def __init__(self, word_embed_size, hidden_size, word2ind, unkToken, padToken, char_embed_size, filter_size=5, dropoutrate=0.3, padding=1):\n",
    "        super(CharCNNLSTMLanguageModelPack, self).__init__()\n",
    "        self.word2ind = word2ind\n",
    "        self.unkTokenIdx = word2ind[unkToken]\n",
    "        self.padTokenIdx = word2ind[padToken]\n",
    "\n",
    "        self.charEmbedding = CharEmbedding(word2ind, char_embed_size, word_embed_size, filter_size, dropoutrate, padding)\n",
    "        self.lstm = torch.nn.LSTM(word_embed_size, hidden_size)\n",
    "        self.projection = torch.nn.Linear(hidden_size,len(word2ind))\n",
    "    \n",
    "    def preparePaddedBatch(self, source):\n",
    "        device = next(self.parameters()).device\n",
    "        m = max(len(s) for s in source)\n",
    "        sents = [[self.word2ind.get(w,self.unkTokenIdx) for w in s] for s in source]\n",
    "        sents_padded = [ s+(m-len(s))*[self.padTokenIdx] for s in sents]\n",
    "        return torch.t(torch.tensor(sents_padded, dtype=torch.long, device=device))\n",
    "    \n",
    "    def forward(self, source):\n",
    "        X = self.preparePaddedBatch(source)\n",
    "        E = self.charEmbedding(source)\n",
    "        source_lengths = [len(s)-1 for s in source]\n",
    "        outputPacked, _ = self.lstm(torch.nn.utils.rnn.pack_padded_sequence(E, source_lengths,enforce_sorted=False))\n",
    "        output,_ = torch.nn.utils.rnn.pad_packed_sequence(outputPacked)\n",
    "        \n",
    "        Z = self.projection(output.flatten(0,1))\n",
    "        Y_bar = X[1:].flatten(0,1)\n",
    "        H = torch.nn.functional.cross_entropy(Z,Y_bar,ignore_index=self.padTokenIdx)\n",
    "        return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "clm = CharCNNLSTMLanguageModelPack(256, 256, word2ind, unkToken, padToken, 32).to(device)\n",
    "optimizer = torch.optim.Adam(clm.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(len(trainCorpus), dtype='int32')\n",
    "np.random.shuffle(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clm.train()\n",
    "for b in range(0, len(idx), batchSize):\n",
    "    batch = [ trainCorpus[i] for i in idx[b:min(b+batchSize, len(idx))] ]\n",
    "    H = clm(batch)\n",
    "    optimizer.zero_grad()\n",
    "    H.backward()\n",
    "    optimizer.step()\n",
    "    if b % 10 == 0:\n",
    "        print(b, '/', len(idx), H.item())\n",
    "clm.eval()\n",
    "perplexity(clm, testCorpus, batchSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Двупосочен LSTM с пакетиране на партида"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharCNNBiLSTMLanguageModelPack(torch.nn.Module):\n",
    "    def __init__(self, word_embed_size, hidden_size, word2ind, unkToken, padToken, endToken, char_embed_size, filter_size=5, dropoutrate=0.3, padding=1):\n",
    "        super(CharCNNBiLSTMLanguageModelPack, self).__init__()\n",
    "        self.word2ind = word2ind\n",
    "        self.unkTokenIdx = word2ind[unkToken]\n",
    "        self.padTokenIdx = word2ind[padToken]\n",
    "        self.endTokenIdx = word2ind[endToken]\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.charEmbedding = CharEmbedding(word2ind, char_embed_size, word_embed_size, filter_size, dropoutrate, padding)\n",
    "        self.lstm = torch.nn.LSTM(word_embed_size, hidden_size, bidirectional=True)\n",
    "        self.projection = torch.nn.Linear(2*hidden_size,len(word2ind))\n",
    "    \n",
    "    def preparePaddedBatch(self, source):\n",
    "        device = next(self.parameters()).device\n",
    "        m = max(len(s) for s in source)\n",
    "        sents = [[self.word2ind.get(w,self.unkTokenIdx) for w in s] for s in source]\n",
    "        sents_padded = [ s+(m-len(s))*[self.padTokenIdx] for s in sents]\n",
    "        return torch.t(torch.tensor(sents_padded, dtype=torch.long, device=device))\n",
    "    \n",
    "    def forward(self, source):\n",
    "        batch_size = len(source)\n",
    "        X = self.preparePaddedBatch(source)\n",
    "        E = self.charEmbedding(source)\n",
    "\n",
    "        source_lengths = [len(s) for s in source]\n",
    "        m = X.shape[0]\n",
    "        outputPacked, _ = self.lstm(torch.nn.utils.rnn.pack_padded_sequence(E, source_lengths,enforce_sorted=False))\n",
    "        \n",
    "        output,_ = torch.nn.utils.rnn.pad_packed_sequence(outputPacked)\n",
    "        output = output.view(m, batch_size, 2, self.hidden_size)\n",
    "        t = torch.cat((output[:-2,:,0,:], output[2:,:,1,:]),2)\n",
    "        Z = self.projection(t.flatten(0,1))\n",
    "        \n",
    "        Y_bar = X[1:-1].flatten(0,1)\n",
    "        Y_bar[Y_bar==self.endTokenIdx] = self.padTokenIdx\n",
    "        H = torch.nn.functional.cross_entropy(Z,Y_bar,ignore_index=self.padTokenIdx)\n",
    "        return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cblm = CharCNNBiLSTMLanguageModelPack(256, 256, word2ind, unkToken, padToken, endToken, 32).to(device)\n",
    "optimizer = torch.optim.Adam(cblm.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(len(trainCorpus), dtype='int32')\n",
    "np.random.shuffle(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cblm.train()\n",
    "for b in range(0, len(idx), batchSize):\n",
    "    batch = [ trainCorpus[i] for i in idx[b:min(b+batchSize, len(idx))] ]\n",
    "    H = cblm(batch)\n",
    "    optimizer.zero_grad()\n",
    "    H.backward()\n",
    "    optimizer.step()\n",
    "    if b % 10 == 0:\n",
    "        print(b, '/', len(idx), H.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.162559473913722"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(cblm, testCorpus, batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
