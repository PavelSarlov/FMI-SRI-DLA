{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Търсене и извличане на информация. Приложение на дълбоко машинно обучение\n",
    "> ### Стоян Михов\n",
    "> #### Зимен семестър 2021/2022\n",
    "\n",
    "### Упражнение 11\n",
    "\n",
    " За да работи програмата трябва корпуса от публицистични текстове за Югоизточна Европа,\n",
    " да се намира разархивиран в директорията, в която е програмата (виж упражнение 2).\n",
    "\n",
    " Преди да се стартира програмата е необходимо да се активира съответното обкръжение с командата: `conda activate tii`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[1.2,2,3],[4,5,6]]);X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.tensor([[3,2,1],[2,3,4.1]], requires_grad=True);Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand(3,4, requires_grad=True);A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = torch.matmul(X+Y,A);B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.sum(-2 * B);C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.grad)\n",
    "print(Y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = torch.mean(A);S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.grad)\n",
    "print(Y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.grad = None\n",
    "Y.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = torch.mean(torch.matmul(Y,A));S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.grad)\n",
    "print(Y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(A,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    A -= 1.2 * A.grad\n",
    "    Y -= 1.2 * Y.grad\n",
    "    A.grad = None\n",
    "    Y.grad = None\n",
    "\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Визуализация на прогреса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class progressBar:\n",
    "    def __init__(self ,barWidth = 50):\n",
    "        self.barWidth = barWidth\n",
    "        self.period = None\n",
    "    def start(self, count):\n",
    "        self.item=0\n",
    "        self.period = int(count / self.barWidth)\n",
    "        sys.stdout.write(\"[\"+(\" \" * self.barWidth)+\"]\")\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"\\b\" * (self.barWidth+1))\n",
    "    def tick(self):\n",
    "        if self.item>0 and self.item % self.period == 0:\n",
    "            sys.stdout.write(\"-\")\n",
    "            sys.stdout.flush()\n",
    "        self.item += 1\n",
    "    def stop(self):\n",
    "        sys.stdout.write(\"]\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractDictionary(corpus, limit=20000):\n",
    "    pb = progressBar()\n",
    "    pb.start(len(corpus))\n",
    "    dictionary = {}\n",
    "    for doc in corpus:\n",
    "        pb.tick()\n",
    "        for w in doc:\n",
    "            if w not in dictionary: dictionary[w] = 0\n",
    "        dictionary[w] += 1\n",
    "    L = sorted([(w,dictionary[w]) for w in dictionary], key = lambda x: x[1] , reverse=True)\n",
    "    if limit > len(L): limit = len(L)\n",
    "    words = [ w for w,_ in L[:limit] ] + [unkToken]\n",
    "    word2ind = { w:i for i,w in enumerate(words)}\n",
    "    pb.stop()\n",
    "    return words, word2ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractData(corpus, order, word2ind):\n",
    "    pb = progressBar()\n",
    "    pb.start(len(corpus))\n",
    "    unk = word2ind[unkToken]\n",
    "    start = word2ind[startToken]\n",
    "\n",
    "    points = sum(len(s)-1 for s in corpus)\n",
    "    \n",
    "    target = np.empty(points, dtype='int32')\n",
    "    context = np.empty((points,order-1), dtype='int32')\n",
    "    p = 0\n",
    "    for doc in corpus:\n",
    "        pb.tick()\n",
    "        for wi in range(1,len(doc)):\n",
    "            i = word2ind.get(doc[wi], unk)\n",
    "            target[p] = i\n",
    "            sample = []\n",
    "            for k in range(1,order):\n",
    "                if wi-k < 0:\n",
    "                    j = start\n",
    "                else:\n",
    "                    j = word2ind.get(doc[wi-k], unk)\n",
    "                context[p,k-1] = j\n",
    "            p += 1\n",
    "    pb.stop()\n",
    "    return target, context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   Зареждане на корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_root = 'JOURNALISM.BG/C-MassMedia'\n",
    "myCorpus = PlaintextCorpusReader(corpus_root, '.*\\.txt')\n",
    "startToken = '<START>'\n",
    "endToken = '<END>'\n",
    "unkToken = '<UNK>'\n",
    "\n",
    "corpus = [ [startToken] + [w.lower() for w in sent] + [endToken] for sent in myCorpus.sents()]\n",
    "\n",
    "words, word2ind = extractDictionary(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 4\n",
    "target, context = extractData(corpus, order, word2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 50\n",
    "hid_size = 100\n",
    "\n",
    "L = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 1000\n",
    "idx = np.arange(len(target), dtype='int32')\n",
    "np.random.shuffle(idx)\n",
    "learning_rate = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   Първи вариант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = torch.empty(L, emb_size, requires_grad = True)\n",
    "W1 = torch.empty((order-1)*emb_size, hid_size, requires_grad = True)\n",
    "b1 = torch.empty(hid_size, requires_grad = True)\n",
    "W2 = torch.empty(hid_size, L, requires_grad = True)\n",
    "b2 = torch.empty(L, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.init.normal_(E)\n",
    "torch.nn.init.normal_(W1)\n",
    "torch.nn.init.normal_(b1)\n",
    "torch.nn.init.normal_(W2)\n",
    "torch.nn.init.normal_(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_fn = torch.nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    E -= learning_rate * E.grad\n",
    "    W1 -= learning_rate * W1.grad\n",
    "    b1 -= learning_rate * b1.grad\n",
    "    W2 -= learning_rate * W2.grad\n",
    "    b2 -= learning_rate * b2.grad\n",
    "    # Manually zero the gradients\n",
    "    E.grad = None\n",
    "    W1.grad = None\n",
    "    b1.grad = None\n",
    "    W2.grad = None\n",
    "    b2.grad = None\n",
    "\n",
    "if b % 10000 == 0:\n",
    "    print(b, '/', len(idx), H.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(0,len(idx),batchSize):\n",
    "\n",
    "    batchIdx = idx[b:min(b+batchSize,len(idx))]\n",
    "    S = len(batchIdx)\n",
    "    batchTarget = torch.tensor(target[batchIdx], dtype=torch.long)\n",
    "    batchContext = context[batchIdx]\n",
    "\n",
    "    X = E[batchContext].view(S,(order-1) * emb_size)\n",
    "    h = sigmoid_fn(torch.matmul(X,W1) + b1)\n",
    "    z = torch.matmul(h,W2) + b2\n",
    "    \n",
    "    t = torch.exp(z)\n",
    "    s = torch.sum(t,axis=1)\n",
    "    z = t/s.unsqueeze(1)\n",
    "    p = z[torch.arange(S),batchTarget]\n",
    "    H = -torch.mean(torch.log(p))\n",
    "\n",
    "    H.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        E -= learning_rate * E.grad\n",
    "        W1 -= learning_rate * W1.grad\n",
    "        b1 -= learning_rate * b1.grad\n",
    "        W2 -= learning_rate * W2.grad\n",
    "        b2 -= learning_rate * b2.grad\n",
    "        # Manually zero the gradients\n",
    "        E.grad = None\n",
    "        W1.grad = None\n",
    "        b1.grad = None\n",
    "        W2.grad = None\n",
    "        b2.grad = None\n",
    "\n",
    "    if b % 10000 == 0:\n",
    "        print(b, '/', len(idx), H.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   Втори вариант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.init.normal_(E)\n",
    "torch.nn.init.normal_(W1)\n",
    "torch.nn.init.normal_(b1)\n",
    "torch.nn.init.normal_(W2)\n",
    "torch.nn.init.normal_(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(0,len(idx),batchSize):\n",
    "    \n",
    "    batchIdx = idx[b:min(b+batchSize,len(idx))]\n",
    "    S = len(batchIdx)\n",
    "    batchTarget = torch.tensor(target[batchIdx], dtype=torch.long)\n",
    "    batchContext = context[batchIdx]\n",
    "    \n",
    "    X = E[batchContext].view(S,(order-1) * emb_size)\n",
    "    h = sigmoid_fn(torch.matmul(X,W1) + b1)\n",
    "    z = torch.matmul(h,W2) + b2\n",
    "    H = loss_fn(z,batchTarget)\n",
    "    \n",
    "    H.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        E -= learning_rate * E.grad\n",
    "        W1 -= learning_rate * W1.grad\n",
    "        b1 -= learning_rate * b1.grad\n",
    "        W2 -= learning_rate * W2.grad\n",
    "        b2 -= learning_rate * b2.grad\n",
    "        # Manually zero the gradients\n",
    "        E.grad = None\n",
    "        W1.grad = None\n",
    "        b1.grad = None\n",
    "        W2.grad = None\n",
    "        b2.grad = None\n",
    "    \n",
    "    if b % 10000 == 0:\n",
    "        print(b, '/', len(idx), H.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   Трети вариант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = torch.empty(L, emb_size, requires_grad = True, device = device)\n",
    "W1 = torch.empty((order-1)*emb_size, hid_size, requires_grad = True, device = device)\n",
    "b1 = torch.empty(hid_size, requires_grad = True, device = device)\n",
    "W2 = torch.empty(hid_size, L, requires_grad = True, device = device)\n",
    "b2 = torch.empty(L, requires_grad = True, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.init.normal_(E)\n",
    "torch.nn.init.normal_(W1)\n",
    "torch.nn.init.normal_(b1)\n",
    "torch.nn.init.normal_(W2)\n",
    "torch.nn.init.normal_(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(0,len(idx),batchSize):\n",
    "    \n",
    "    batchIdx = idx[b:min(b+batchSize,len(idx))]\n",
    "    S = len(batchIdx)\n",
    "    batchTarget = torch.tensor(target[batchIdx], dtype=torch.long, device = device)\n",
    "    batchContext = context[batchIdx]\n",
    "    \n",
    "    X = E[batchContext].view(S,(order-1) * emb_size)\n",
    "    h = sigmoid_fn(torch.matmul(X,W1) + b1)\n",
    "    z = torch.matmul(h,W2) + b2\n",
    "    H = loss_fn(z,batchTarget)\n",
    "    \n",
    "    H.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        E -= learning_rate * E.grad\n",
    "        W1 -= learning_rate * W1.grad\n",
    "        b1 -= learning_rate * b1.grad\n",
    "        W2 -= learning_rate * W2.grad\n",
    "        b2 -= learning_rate * b2.grad\n",
    "        # Manually zero the gradients\n",
    "        E.grad = None\n",
    "        W1.grad = None\n",
    "        b1.grad = None\n",
    "        W2.grad = None\n",
    "        b2.grad = None\n",
    "    \n",
    "    if b % 10000 == 0:\n",
    "        print(b, '/', len(idx), H.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
